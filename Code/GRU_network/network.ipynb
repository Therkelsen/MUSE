{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Libaries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install pandas\n",
    "# %pip install scikit-learn\n",
    "# %pip install plotly\n",
    "# %pip install scipy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "from keras.models import Sequential\n",
    "from keras import Input\n",
    "from keras.layers import Bidirectional, GRU, RepeatVector, Dense, TimeDistributed, concatenate, Dot, Activation, Concatenate, Flatten # for creating layers inside the Neural Network\n",
    "\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "\n",
    "import sklearn\n",
    "print('sklearn: %s' % sklearn.__version__)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "print('plotly: %s' % plotly.__version__)\n",
    "\n",
    "import scipy.io\n",
    "from scipy.interpolate import interp1d\n",
    "print('scipy: %s' % scipy.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>From several csv files</h3>\n",
    "This section synchronizes and concatinates the EIM and kinematic data through unix time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_mean(input_signal):\n",
    "    output_signal = input_signal.copy()\n",
    "    buffer = len(input_signal) // 50\n",
    "    running_sum = 0.0\n",
    "\n",
    "    for i in range(len(input_signal)):\n",
    "        running_sum += input_signal[i]\n",
    "\n",
    "        if i < buffer:\n",
    "            output_signal[i] = running_sum / float(i + 1)\n",
    "        else:\n",
    "            running_sum -= input_signal[i - buffer]\n",
    "            output_signal[i] = running_sum / float(buffer)\n",
    "\n",
    "    return output_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas options to display more columns\n",
    "pd.options.display.max_columns=150\n",
    "\n",
    "# Loading data and adding headers 'ElbowAngles' and 'Time'\n",
    "kinematic_data_dir = 'MoCap/dynamic/99_HRML_4kg_pd/elbow_angles.csv'\n",
    "eim_data_dir = 'MoCap/dynamic/99_HRML_4kg_pd/processed_output_data.csv'\n",
    "df_kin=pd.read_csv(kinematic_data_dir, encoding='utf-8', names=[\"ElbowAngles\", \"Time\"])\n",
    "df_eim=pd.read_csv(eim_data_dir, encoding='utf-8')\n",
    "\n",
    "# Stripping data of unwanted delimiters and converting to float\n",
    "df_kin['ElbowAngles'] = df_kin['ElbowAngles'].str.strip('[]').astype(float)\n",
    "df_kin['Time'] = df_kin['Time'].str.strip('[]').astype(str)\n",
    "df_kin['Time'] = df_kin['Time'].str.strip('\\'').astype(float)\n",
    "\n",
    "# Extracting Unix time\n",
    "kin_unix = df_kin['Time'].values\n",
    "eim_unix = df_eim['Time'].values\n",
    "\n",
    "# Extracting max and min Unix values of kin and using these to figure out where to slice the EIM data\n",
    "kin_min = kin_unix.min()\n",
    "kin_max = kin_unix.max()\n",
    "\n",
    "# Filter EIM data based on kinematic Unix time range\n",
    "df_eim = df_eim[(df_eim['Time'] >= kin_min) & (df_eim['Time'] <= kin_max)]\n",
    "\n",
    "# Creating timestamps. EIM samples at 1000hZ, so 1 timestamp will correspond to 1ms\n",
    "df_eim['Timestamp'] = np.linspace(0, (len(df_eim) - 1), len(df_eim))\n",
    "df_kin['Timestamp'] = np.linspace(0, (len(df_kin) - 1), len(df_kin))\n",
    "\n",
    "# Extract timestamps and kinematic data\n",
    "kinematic_timestamps = df_kin['Timestamp'].values\n",
    "kinematic_data = df_kin['ElbowAngles'].values\n",
    "eim_timestamps = df_eim['Timestamp'].values\n",
    "\n",
    "# Interpolation of the kinematic data to match the EIM data\n",
    "shape = eim_timestamps.shape\n",
    "kinematic_interpolated = np.empty(shape)\n",
    "kin_idx = 0\n",
    "step = math.floor(len(eim_timestamps)/len(kinematic_timestamps))\n",
    "step_remainder = len(eim_timestamps)/len(kinematic_timestamps) - step\n",
    "step_temp = 0\n",
    "temp = 0\n",
    "\n",
    "# Linear interpolation done by averaging between two points. Each data step is done based \n",
    "# on the integer difference between the lenghts of the datasets. For increased accuracy, \n",
    "# whenever the remainder of the division becomes equal to or greater than 1, 1 is added \n",
    "# to the step, and withdrawn from the counter.\n",
    "for i in range(0, len(kinematic_data), 1):\n",
    "    kinematic_interpolated[kin_idx] = kinematic_data[i]\n",
    "    if i < len(kinematic_data)-1:\n",
    "        temp = kinematic_data[i+1]\n",
    "        for j in range(kin_idx + 1, kin_idx+step, 1):\n",
    "            kinematic_interpolated[j] = (kinematic_interpolated[j-1] + temp) / 2\n",
    "    else:\n",
    "        temp = kinematic_data[i]\n",
    "        for j in range(kin_idx + 1, len(eim_timestamps), 1):\n",
    "            kinematic_interpolated[j] = (kinematic_interpolated[j-1] + temp) / 2\n",
    "\n",
    "        step_temp = step_temp+step_remainder\n",
    "    if step_temp >= 1:\n",
    "        kin_idx = kin_idx + step + 1\n",
    "        step_temp = step_temp - 1\n",
    "    else:\n",
    "        kin_idx = kin_idx + step\n",
    "\n",
    "\n",
    "# Interpolate kinematic data to match EIM timestamps. Lines are drawn between the spread out data. \n",
    "# Missing values are being extrapolated\n",
    "kinematic_interpolated = interp1d(eim_timestamps, kinematic_interpolated, kind='linear', fill_value='extrapolate')\n",
    "\n",
    "# The interpolated data is being saved to the JointAngle column in the eim_data\n",
    "df_eim['JointAngle'] = kinematic_interpolated(eim_timestamps)\n",
    "\n",
    "# df_eim['RollingAverageMag'] = df_eim['EIMMagnitude'].rolling(100).mean()\n",
    "# df_eim['RollingAveragePhase'] = df_eim['EIMPhase'].rolling(100).mean()\n",
    "\n",
    "# Calculating rolling mean\n",
    "\n",
    "df_eim['RollingAverageMag'] = rolling_mean(df_eim['EIMMagnitude'])\n",
    "df_eim['RollingAveragePhase'] = rolling_mean(df_eim['EIMPhase'])\n",
    "df_eim['RollingStdEIM'] = df_eim['EIMMagnitude'].rolling(100).std()\n",
    "std_value=df_eim['RollingStdEIM'][99]\n",
    "df_eim['RollingStdEIM'].fillna(value=std_value, inplace=True) \n",
    "\n",
    "#Filling NaN values out with the first mean value in the series\n",
    "# mean_value=df_eim['RollingAverageMag'][99]\n",
    "# df_eim['RollingAverageMag'].fillna(value=mean_value, inplace=True) \n",
    "# mean_value=df_eim['RollingAveragePhase'][99]\n",
    "# df_eim['RollingAveragePhase'].fillna(value=mean_value, inplace=True) \n",
    "\n",
    "\n",
    "# NB! THIS SECTION OF SAVING THE FILES INTO ONE CSV FILE HAS BEEN COMMENTED OUT, NOT \n",
    "# TO RISK OVERWRITING THE all_samples.csv FILE\n",
    "\n",
    "# Saving the result to csv, where all samples are gonna be saved\n",
    "# df_final = df_eim[['Sample', 'EIMMagnitude', 'EIMPhase', 'JointAngle', 'Mass', 'Time', 'RollingAverageMag', 'RollingAveragePhase']]\n",
    "# df_eim_kin = 'all_samples.csv'\n",
    "# \n",
    "# if(os.path.isfile(df_eim_kin)):\n",
    "#     sample_number = input(\"Please provide the sample number and press enter:\")\n",
    "#     mass = input(\"Please provide the mass used in the current sample:\")\n",
    "#     df_final = df_final.assign(Sample=sample_number)\n",
    "#     df_final = df_final.assign(Mass=mass)\n",
    "#     # df_final.loc[:]['Sample'] = sample_number\n",
    "#     # df_final.loc[:]['Mass'] = mass\n",
    "#     # df_final['Sample'] = df_final['Sample'].replace(df_final['Sample'], sample_number)\n",
    "#     # df_final['Mass'] = df_final['Mass'].replace(df_final['Mass'], sample_number)\n",
    "\n",
    "#     df_final.to_csv(df_eim_kin, mode='a', index= False, header=False)\n",
    "# else:\n",
    "#     sample_number = input(\"Please provide the sample number and press enter:\")\n",
    "#     mass = input(\"Please provide the mass used in the current sample:\")\n",
    "#     df_final = df_final.assign(Sample=sample_number)\n",
    "#     df_final = df_final.assign(Mass=mass)\n",
    "#     # df_final['Sample'] = df_final['Sample'].replace(df_final['Sample'], sample_number)\n",
    "#     # df_final['Mass'] = df_final['Mass'].replace(df_final['Mass'], sample_number)\n",
    "\n",
    "#     df_final.to_csv(df_eim_kin, mode='w', index= False)\n",
    "\n",
    "# Assuming df_eim is your DataFrame\n",
    "plt.plot(df_eim['Timestamp'], df_eim['JointAngle'], marker='o')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('ElbowAngles')\n",
    "plt.title('Line Plot of ElbowAngles')\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(df_eim['Timestamp'], df_eim['EIMMagnitude'], marker='o')\n",
    "# plt.xlabel('Timestamp')\n",
    "# plt.ylabel('EIMMagnitude')\n",
    "# plt.title('Line Plot of EIMMagnitude')\n",
    "# plt.show()\n",
    "\n",
    "plt.plot(df_eim['Timestamp'], df_eim['RollingAverageMag'], marker='o')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('RollingAverageEIM')\n",
    "plt.title('Line Plot of RollingAverageEIM')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(df_eim['Timestamp'], df_eim['RollingStdEIM'], marker='o')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('RollingStdEIM')\n",
    "plt.title('Line Plot of RollingStdEIM')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(df_eim['Timestamp'], df_eim['RollingAveragePhase'], marker='o')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('EIMPhase')\n",
    "plt.title('Line Plot of EIMPhase')\n",
    "plt.show()\n",
    "\n",
    "# print(df_final.head(100))\n",
    "print(df_eim.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>From one csv file</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load csv of all samples. Group data by 'Sample'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_all = 'all_samples.csv'\n",
    "df_all=pd.read_csv(dir_all, encoding='utf-8')\n",
    "grouped = df_all.groupby('Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas options to display more columns\n",
    "pd.options.display.max_columns=150\n",
    "\n",
    "# Calculate the median for each group\n",
    "median_values = grouped[['EIMMagnitude', 'EIMPhase']].apply(lambda group: group[['EIMMagnitude', 'EIMPhase']].agg(['min', 'max']).median())\n",
    "\n",
    "# Calculate the mean for each group\n",
    "mean_values = grouped[['EIMMagnitude', 'EIMPhase']].mean()\n",
    "\n",
    "# Calculate the standard deviation for each group\n",
    "standard_deviations = grouped[['EIMMagnitude', 'EIMPhase']].std()\n",
    "\n",
    "# Calculate the variance for each group\n",
    "variance_values = grouped[['EIMMagnitude', 'EIMPhase']].var()\n",
    "\n",
    "# Calculate the kurtosis for each group\n",
    "kurtosis_values = grouped[['EIMMagnitude', 'EIMPhase']].apply(pd.DataFrame.kurtosis)\n",
    "\n",
    "# Reset the index to get the 'Sample' column back\n",
    "median_values.reset_index(inplace=True)\n",
    "mean_values.reset_index(inplace=True)\n",
    "standard_deviations.reset_index(inplace=True)\n",
    "variance_values.reset_index(inplace=True)\n",
    "kurtosis_values.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns to indicate they represent the median\n",
    "median_values.columns = ['Sample', 'MedianEIMMagnitude', 'MedianEIMPhase']\n",
    "mean_values.columns = ['Sample', 'MeanEIMMagnitude', 'MeanEIMPhase']\n",
    "standard_deviations.columns = ['Sample', 'StdEIMMagnitude', 'StdEIMPhase']\n",
    "variance_values.columns = ['Sample', 'VarEIMMagnitude', 'VarEIMPhase']\n",
    "kurtosis_values.columns = ['Sample', 'KurtEIMMagnitude', 'KurtEIMPhase']\n",
    "\n",
    "# Merge the median and mean values back into the original DataFrame based on the 'Sample' column\n",
    "df_all = df_all.merge(median_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(mean_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(standard_deviations, on='Sample', how='left')\n",
    "df_all = df_all.merge(variance_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(kurtosis_values, on='Sample', how='left')\n",
    "\n",
    "# Calculate rate of change for each group\n",
    "df_all['ROCEIMMagnitude'] = df_all['RollingAverageMag'].pct_change()\n",
    "df_all['ROCEIMPhase'] = df_all['RollingAveragePhase'].pct_change()\n",
    "\n",
    "#Filling NaN values out with the first mean value in the series\n",
    "ROC_value=df_all['ROCEIMMagnitude'][1]\n",
    "df_all['ROCEIMMagnitude'].fillna(value=ROC_value, inplace=True)\n",
    "ROC_value=df_all['ROCEIMPhase'][1]\n",
    "df_all['ROCEIMPhase'].fillna(value=ROC_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming your data is stored in a pandas DataFrame named 'df'\n",
    "# Filter the data for the first 10 samples\n",
    "df_last_10_samples = df_all[df_all['Sample'] >= 89]\n",
    "\n",
    "# Create a scatter plot for EIMMagnitude and JointAngle\n",
    "fig = go.Figure()\n",
    "\n",
    "for sample in df_last_10_samples['Sample'].unique():\n",
    "    sample_data = df_last_10_samples[df_last_10_samples['Sample'] == sample]\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['EIMMagnitude'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - EIMMagnitude'))\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['JointAngle'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - JointAngle'))\n",
    "\n",
    "fig.update_layout(title='EMG and Kinematic Data for the last 10 Samples',\n",
    "                  xaxis_title='Time Steps', yaxis_title='Value')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create test set<h3>\n",
    "These will be extracted from dataframe after normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_all.groupby(['Sample'])\n",
    "\n",
    "specific_samples = [23, 38, 47, 50, 89, 98]\n",
    "\n",
    "# Create an empty DataFrame to store the selected samples\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "# Iterate through the specific samples and extract them\n",
    "for sample_value in specific_samples:\n",
    "    if sample_value in df_grouped.groups:\n",
    "        df_test = pd.concat([df_test, df_grouped.get_group(sample_value)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_dir = 'test_samples.csv'\n",
    "# df_test.to_csv(df_test_dir, index= False)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns you want to normalize\n",
    "features_to_normalize = ['EIMMagnitude', 'EIMPhase', 'RollingAverageMag', 'RollingAveragePhase',\n",
    "                        'MedianEIMMagnitude', 'MedianEIMPhase', 'MeanEIMMagnitude',\n",
    "                        'MeanEIMPhase', 'StdEIMMagnitude', 'StdEIMPhase', 'VarEIMMagnitude',\n",
    "                        'VarEIMPhase', 'KurtEIMMagnitude', 'KurtEIMPhase', 'ROCEIMMagnitude',\n",
    "                        'ROCEIMPhase']\n",
    "\n",
    "targets_to_normalize = ['Mass', 'JointAngle']\n",
    "\n",
    "columns_to_use = ['EIMMagnitude', 'EIMPhase', 'RollingAverageMag', 'RollingAveragePhase',\n",
    "                        'MedianEIMMagnitude', 'MedianEIMPhase', 'MeanEIMMagnitude',\n",
    "                        'MeanEIMPhase', 'StdEIMMagnitude', 'StdEIMPhase', 'VarEIMMagnitude',\n",
    "                        'VarEIMPhase', 'KurtEIMMagnitude', 'KurtEIMPhase', 'ROCEIMMagnitude',\n",
    "                        'ROCEIMPhase', 'Mass', 'JointAngle']\n",
    "\n",
    "# Extract the 'Sample' column to append after normalization\n",
    "sample_column = df_all['Sample'].values\n",
    "\n",
    "# Create a copy of an original dataframe\n",
    "df2=df_all.drop(['Time', 'Sample'], axis=1)\n",
    "\n",
    "# Extracting mean and standard deviation for mean normalization\n",
    "df_mean = df2.mean()\n",
    "df_std = df2.std()\n",
    "normalized_df=(df2-df_mean)/df_std\n",
    "\n",
    "# Add the sample column again\n",
    "normalized_df['Sample'] = sample_column\n",
    "\n",
    "# Save means and std to CSV\n",
    "# df_mean.to_csv('/content/drive/MyDrive/NeuralNetwork/means.csv', header=True)\n",
    "# df_std.to_csv('/content/drive/MyDrive/NeuralNetwork/std_devs.csv', header=True)\n",
    "\n",
    "# Show a snapshot of data\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org = normalized_df*df_std+df_mean\n",
    "df_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original DataFrame length\n",
    "print(\"Original DataFrame Length:\", len(normalized_df))\n",
    "\n",
    "# Remove the extracted samples from the original DataFrame\n",
    "normalized_df.drop(normalized_df[normalized_df['Sample'] == 23].index, inplace = True)\n",
    "normalized_df.drop(normalized_df[normalized_df['Sample'] == 38].index, inplace = True)\n",
    "normalized_df.drop(normalized_df[normalized_df['Sample'] == 47].index, inplace = True)\n",
    "normalized_df.drop(normalized_df[normalized_df['Sample'] == 50].index, inplace = True)\n",
    "normalized_df.drop(normalized_df[normalized_df['Sample'] == 89].index, inplace = True)\n",
    "normalized_df.drop(normalized_df[normalized_df['Sample'] == 98].index, inplace = True)\n",
    "\n",
    "# Display the modified original DataFrame length\n",
    "print(\"\\nModified Original DataFrame Length:\", len(normalized_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if test samples have been removed\n",
    "for sample_value in specific_samples:\n",
    "  print(sample_value in normalized_df['Sample'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of features and sequence lengths\n",
    "num_features = 16\n",
    "sequence_length = 10\n",
    "\n",
    "columns_x = ['EIMMagnitude', 'EIMPhase', 'RollingAverageMag', 'RollingAveragePhase',\n",
    "            'MedianEIMMagnitude', 'MedianEIMPhase', 'MeanEIMMagnitude',\n",
    "            'MeanEIMPhase', 'StdEIMMagnitude', 'StdEIMPhase', 'VarEIMMagnitude',\n",
    "            'VarEIMPhase', 'KurtEIMMagnitude', 'KurtEIMPhase', 'ROCEIMMagnitude',\n",
    "            'ROCEIMPhase']\n",
    "\n",
    "columns_y = ['JointAngle', 'Mass']\n",
    "\n",
    "# Group the data by the 'Sample' column\n",
    "df_grouped = normalized_df.groupby(['Sample'])\n",
    "\n",
    "# Create sequences for each group\n",
    "X_seq, y_seq = [], []\n",
    "\n",
    "for group_name, group_data in df_grouped:\n",
    "\n",
    "    group_data_x_temp = group_data[columns_x]\n",
    "    # Convert to NumPy array\n",
    "    group_data_x = np.array(group_data_x_temp)\n",
    "\n",
    "    group_data_temp_y = group_data[columns_y]\n",
    "    # Convert to NumPy array\n",
    "    group_data_y = np.array(group_data_temp_y)\n",
    "\n",
    "    # Create sequences\n",
    "    for i in range(len(group_data) - sequence_length + 1):\n",
    "        X_seq.append(group_data_x[i:i+sequence_length, :])\n",
    "        y_seq.append(group_data_y[i+sequence_length-1, :])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_seq_np = np.array(X_seq)\n",
    "y_seq_np = np.array(y_seq)\n",
    "\n",
    "# Split into training and validation data in a 80/20 ratio. Testing is done with the samples extracted from the dataset earlier\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_seq_np, y_seq_np, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature engineering</h1> \n",
    "Calculating median, mean, standard deviation, variance and kurtosis for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas options to display more columns\n",
    "pd.options.display.max_columns=150\n",
    "\n",
    "# Calculate the median for each group\n",
    "median_values = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].apply(lambda group: group[['EIMMagnitude', 'EIMPhase', 'JointAngle']].agg(['min', 'max']).median())\n",
    "\n",
    "# Calculate the mean for each group\n",
    "mean_values = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].mean()\n",
    "\n",
    "# Calculate the standard deviation for each group\n",
    "standard_deviations = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].std()\n",
    "\n",
    "# Calculate the variance for each group\n",
    "variance_values = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].var()\n",
    "\n",
    "# Calculate the kurtosis for each group\n",
    "kurtosis_values = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].apply(pd.DataFrame.kurtosis)\n",
    "\n",
    "# Reset the index to get the 'Sample' column back\n",
    "median_values.reset_index(inplace=True)\n",
    "mean_values.reset_index(inplace=True)\n",
    "standard_deviations.reset_index(inplace=True)\n",
    "variance_values.reset_index(inplace=True)\n",
    "kurtosis_values.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns to indicate they represent the median\n",
    "median_values.columns = ['Sample', 'MedianEIMMagnitude', 'MedianEIMPhase', 'MedianJointAngle']\n",
    "mean_values.columns = ['Sample', 'MeanEIMMagnitude', 'MeanEIMPhase', 'MeanJointAngle']\n",
    "standard_deviations.columns = ['Sample', 'StdEIMMagnitude', 'StdEIMPhase', 'StdJointAngle']\n",
    "variance_values.columns = ['Sample', 'VarEIMMagnitude', 'VarEIMPhase', 'VarJointAngle']\n",
    "kurtosis_values.columns = ['Sample', 'KurtEIMMagnitude', 'KurtEIMPhase', 'KurtJointAngle']\n",
    "\n",
    "# Merge the median and mean values back into the original DataFrame based on the 'Sample' column\n",
    "df_all = df_all.merge(median_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(mean_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(standard_deviations, on='Sample', how='left')\n",
    "df_all = df_all.merge(variance_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(kurtosis_values, on='Sample', how='left')\n",
    "\n",
    "# Calculate rate of change for each group\n",
    "df_all['ROCEIMMagnitude'] = df_all['RollingAverageMag'].pct_change()\n",
    "df_all['ROCEIMPhase'] = df_all['RollingAveragePhase'].pct_change()\n",
    "df_all['ROCJointAngle'] = df_all['JointAngle'].pct_change()\n",
    "\n",
    "#Filling NaN values out with the first mean value in the series\n",
    "ROC_value=df_all['ROCEIMMagnitude'][1]\n",
    "df_all['ROCEIMMagnitude'].fillna(value=ROC_value, inplace=True)\n",
    "ROC_value=df_all['ROCEIMPhase'][1]\n",
    "df_all['ROCEIMPhase'].fillna(value=ROC_value, inplace=True)\n",
    "ROC_value=df_all['ROCJointAngle'][1]\n",
    "df_all['ROCJointAngle'].fillna(value=ROC_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Visualization</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming your data is stored in a pandas DataFrame named 'df'\n",
    "# Filter the data for the first 10 samples\n",
    "df_last_10_samples = df_all[df_all['Sample'] >= 89]\n",
    "\n",
    "# Create a scatter plot for EIMMagnitude and JointAngle\n",
    "fig = go.Figure()\n",
    "\n",
    "for sample in df_last_10_samples['Sample'].unique():\n",
    "    sample_data = df_last_10_samples[df_last_10_samples['Sample'] == sample]\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['EIMMagnitude'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - EIMMagnitude'))\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['JointAngle'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - JointAngle'))\n",
    "\n",
    "fig.update_layout(title='EMG and Kinematic Data for the last 10 Samples',\n",
    "                  xaxis_title='Time Steps', yaxis_title='Value')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data preprocessing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing. Range of the scaler should be the same for x and y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of an original dataframe\n",
    "df2=df_all.copy()\n",
    "\n",
    "# Define the columns you want to normalize\n",
    "columns_to_normalize = ['EIMMagnitude', 'EIMPhase',\t'JointAngle', 'RollingAverageMag', 'RollingAveragePhase', \n",
    "                        'MedianEIMMagnitude', 'MedianEIMPhase', 'MedianJointAngle', 'MeanEIMMagnitude',\n",
    "                        'MeanEIMPhase', 'MeanJointAngle', 'StdEIMMagnitude', 'StdEIMPhase',\t'StdJointAngle',\n",
    "                        'VarEIMMagnitude', 'VarEIMPhase', 'VarJointAngle', 'KurtEIMMagnitude', 'KurtEIMPhase',\n",
    "                        'KurtJointAngle', 'ROCEIMMagnitude', 'ROCEIMPhase', 'ROCJointAngle']\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "df2[columns_to_normalize] = scaler.fit_transform(df_all[columns_to_normalize])\n",
    "\n",
    "# Show a snaphsot of data\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df2.groupby(['Sample'])\n",
    "\n",
    "# Display the original DataFrame length\n",
    "print(\"Original DataFrame Length:\", len(df2))\n",
    "\n",
    "specific_samples = [23, 38, 47, 50, 89, 98]\n",
    "\n",
    "# Create an empty DataFrame to store the selected samples\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "# Iterate through the specific samples and extract them\n",
    "for sample_value in specific_samples:\n",
    "    if sample_value in df_grouped.groups:\n",
    "        df_test = pd.concat([df_test, df_grouped.get_group(sample_value)])\n",
    "\n",
    "# Remove the extracted samples from the original DataFrame\n",
    "df2.drop(df2[df2['Sample'] == 23].index, inplace = True)\n",
    "df2.drop(df2[df2['Sample'] == 38].index, inplace = True)\n",
    "df2.drop(df2[df2['Sample'] == 47].index, inplace = True)\n",
    "df2.drop(df2[df2['Sample'] == 50].index, inplace = True)\n",
    "df2.drop(df2[df2['Sample'] == 89].index, inplace = True)\n",
    "df2.drop(df2[df2['Sample'] == 98].index, inplace = True)\n",
    "\n",
    "# Display the modified original DataFrame length\n",
    "print(\"\\nModified Original DataFrame Length:\", len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_value in specific_samples:\n",
    "  print(sample_value in df2['Sample'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data is stored in a pandas DataFrame named 'df'\n",
    "# Filter the data for the first 10 samples\n",
    "df_last_10_samples = df2[df2['Sample'] >= 89]\n",
    "\n",
    "# Create a scatter plot for EIMMagnitude and JointAngle\n",
    "fig = go.Figure()\n",
    "\n",
    "for sample in df_last_10_samples['Sample'].unique():\n",
    "    sample_data = df_last_10_samples[df_last_10_samples['Sample'] == sample]\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['EIMMagnitude'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - EIMMagnitude'))\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['JointAngle'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - JointAngle'))\n",
    "\n",
    "fig.update_layout(title='EMG and Kinematic Data for the First 10 Samples',\n",
    "                  xaxis_title='Time Steps', yaxis_title='Value')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prepare sequences for GRU</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important for synchronizing the data with eachother with a common time variable. The \"time-variable\" from the dataset is therefor strictly speaking not needed for training the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training and evaluating GRU model.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_gru(datain, timestep, scaler, columns_to_use):\n",
    "    X_out, Y_out = None, None\n",
    "\n",
    "    for sample in datain['Sample'].unique():\n",
    "        datatmp = datain[datain['Sample'] == sample].copy()\n",
    "\n",
    "        for col in columns_to_use:\n",
    "            arr = datatmp[col].to_numpy()\n",
    "\n",
    "            # Scale using transform (using previously fitted scaler)\n",
    "            arr_scaled = scaler.transform(arr.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Use numpy's stride tricks to create the sequences\n",
    "            shape = (len(arr_scaled) - 2 * timestep + 1, timestep, 1)\n",
    "            strides = (arr_scaled.itemsize, arr_scaled.itemsize, arr_scaled.itemsize)\n",
    "            X_tmp = np.lib.stride_tricks.as_strided(arr_scaled, shape=shape, strides=strides)\n",
    "\n",
    "            # Reshape for GRU input\n",
    "            X_tmp = X_tmp.reshape(-1, timestep, 1)\n",
    "\n",
    "            # Create corresponding Y sequences\n",
    "            Y_tmp = arr_scaled[timestep:2 * timestep].reshape(-1, timestep, 1)\n",
    "\n",
    "            if X_out is None:\n",
    "                X_out = X_tmp\n",
    "                Y_out = Y_tmp\n",
    "            else:\n",
    "                X_out = np.concatenate((X_out, X_tmp), axis=0)\n",
    "                Y_out = np.concatenate((Y_out, Y_tmp), axis=0)\n",
    "\n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data and reshaping it for GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_path = 'EIM_kin_data.csv'\n",
    "# df_test=pd.read_csv(test_path, encoding='utf-8')\n",
    "\n",
    "thousand_datapoints = df_all.iloc[0:4000]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "timestep = 10\n",
    "\n",
    "# Split data into train, eval, and test dataframes\n",
    "train_ratio = 0.7\n",
    "eval_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Split the data\n",
    "df_train, df_temp = train_test_split(thousand_datapoints, test_size=1 - train_ratio, random_state=42)\n",
    "df_eval, df_test = train_test_split(df_temp, test_size=test_ratio / (test_ratio + eval_ratio), random_state=42)\n",
    "\n",
    "# 'EIMMagnitude', 'EIMPhase',\t'JointAngle', 'Mass', 'RollingAverageMag', 'RollingAveragePhase', \n",
    "#                         'MedianEIMMagnitude', 'MedianEIMPhase', 'MedianJointAngle', 'MeanEIMMagnitude',\n",
    "#                         'MeanEIMPhase', 'MeanJointAngle', 'StdEIMMagnitude', 'StdEIMPhase',\t'StdJointAngle',\n",
    "#                         'VarEIMMagnitude', 'VarEIMPhase', 'VarJointAngle', 'KurtEIMMagnitude', 'KurtEIMPhase',\n",
    "#                         'KurtJointAngle', 'ROCEIMMagnitude', 'ROCEIMPhase', 'ROCJointAngle'\n",
    "\n",
    "columns_to_train_on = ['EIMMagnitude', 'EIMPhase',\t'JointAngle', 'Mass', 'RollingAverageMag', 'RollingAveragePhase', \n",
    "                        'MedianEIMMagnitude', 'MedianEIMPhase', 'MedianJointAngle', 'MeanEIMMagnitude']\n",
    "\n",
    "# Use fit to train the scaler on the training data only, actual scaling will be done inside reshaping function\n",
    "scaler.fit(df_train[columns_to_train_on].values.reshape(-1, 1))\n",
    "\n",
    "# Use the reshaping function to reshape the data for GRU\n",
    "columns_to_use = columns_to_train_on\n",
    "\n",
    "X_train, Y_train = reshape_for_gru(df_train, timestep, scaler, columns_to_use)\n",
    "X_eval, Y_eval = reshape_for_gru(df_eval, timestep, scaler, columns_to_use)\n",
    "X_test, Y_test = reshape_for_gru(df_test, timestep, scaler, columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train_shape: ',X_train.shape)\n",
    "print('Y_train_shape: ',Y_train.shape)\n",
    "print('X_eval_shape: ',X_eval.shape)\n",
    "print('Y_eval_shape: ',Y_eval.shape)\n",
    "print('X_test_shape: ',X_test.shape)\n",
    "print('Y_test_shape: ',Y_test.shape)\n",
    "\n",
    "print(Y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Specify the structure of a Neural Network<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name=\"GRU-Model\")\n",
    "model.add(Input(shape=(X_train.shape[1],X_train.shape[2]), name='Input-Layer'))\n",
    "model.add(Bidirectional(GRU(units=32, activation='tanh', recurrent_activation='sigmoid', stateful=False), name='Hidden-GRU-Encoder-Layer'))\n",
    "model.add(RepeatVector(X_train.shape[1], name='Repeat-Vector-Layer'))\n",
    "model.add(Bidirectional(GRU(units=32, activation='tanh', recurrent_activation='sigmoid', stateful=False, return_sequences=True), name='Hidden-GRU-Decoder-Layer'))\n",
    "model.add(TimeDistributed(Dense(units=1, activation='linear'), name='Output-Layer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compile the model<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['MeanSquaredError', 'MeanAbsoluteError'],\n",
    "              loss_weights=None,\n",
    "              weighted_metrics=None,\n",
    "              run_eagerly=None,\n",
    "              steps_per_execution=None\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fit the model on the dataset<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    callbacks=None,\n",
    "                    validation_data=(X_eval, Y_eval),\n",
    "                    shuffle=True,\n",
    "                    class_weight=None,\n",
    "                    sample_weight=None,\n",
    "                    initial_epoch=0,\n",
    "                    steps_per_epoch=None,\n",
    "                    validation_steps=None,\n",
    "                    validation_batch_size=None,\n",
    "                    validation_freq=10,\n",
    "                    max_queue_size=10,\n",
    "                    workers=1,\n",
    "                    use_multiprocessing=True,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use model to make predictions<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict results on training data\n",
    "#pred_train = model.predict(X_train)\n",
    "# Predict results on test data\n",
    "pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Print Performance Summary<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print('-------------------- Model Summary --------------------')\n",
    "# print model summary\n",
    "model.summary()\n",
    "print(\"\")\n",
    "print('-------------------- Weights and Biases --------------------')\n",
    "print(\"Too many parameters to print but you can use the code provided if needed\")\n",
    "print(\"\")\n",
    "#for layer in model.layers:\n",
    "#    print(layer.name)\n",
    "#    for item in layer.get_weights():\n",
    "#        print(\"  \", item)\n",
    "#print(\"\")\n",
    "\n",
    "# Print the last value in the evaluation metrics contained within history file\n",
    "print('-------------------- Evaluation on Training Data --------------------')\n",
    "for item in history.history:\n",
    "    print(\"Final\", item, \":\", history.history[item][-1])\n",
    "print(\"\")\n",
    "\n",
    "# Evaluate the model on the test data using \"evaluate\"\n",
    "print('-------------------- Evaluation on Test Data --------------------')\n",
    "results = model.evaluate(X_test, Y_test)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "\n",
    "# Create a plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for actual values (ground truth)\n",
    "for i in range(X_test):\n",
    "    fig.add_trace(go.Scatter(x=np.arange(len(Y_test[i])), y=scaler.inverse_transform(Y_test[i].reshape(-1, 1)).flatten(),\n",
    "                             mode='lines',\n",
    "                             name=f'Sample {i + 1} - Actual',\n",
    "                             opacity=0.8,\n",
    "                             line=dict(width=1)\n",
    "                            ))\n",
    "\n",
    "# Add traces for predicted values\n",
    "for i in range(X_test):\n",
    "    fig.add_trace(go.Scatter(x=np.arange(len(pred_test[i])), y=scaler.inverse_transform(pred_test[i].reshape(-1, 1)).flatten(),\n",
    "                             mode='lines',\n",
    "                             name=f'Sample {i + 1} - Predicted',\n",
    "                             opacity=1,\n",
    "                             line=dict(width=2, dash='dot')\n",
    "                            ))\n",
    "\n",
    "# Customize the plot appearance\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white',\n",
    "    xaxis=dict(\n",
    "        showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "        zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "        showline=True, linewidth=1, linecolor='black',\n",
    "        title='Time Steps'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "        zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "        showline=True, linewidth=1, linecolor='black',\n",
    "        title='Values'\n",
    "    ),\n",
    "    title=dict(text=\"Actual vs. Predicted Values\", font=dict(color='black'))\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Splitting the data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the data into a training/evaluation/test distribution of 70/20/10. The random_state parameter is a seed for the random split, that allows reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kinematic_features = 18\n",
    "num_emg_features = 7\n",
    "\n",
    "# Extract X_kinematic and y_kinematic\n",
    "# Adjust num_kinematic_features\n",
    "X_kinematic = kinematic_data_normalized[:, :, :num_kinematic_features]\n",
    "# Assuming the last time step represents the target\n",
    "y_kinematic = kinematic_data_normalized[:, -1, :]\n",
    "\n",
    "# Extract X_emg and y_emg\n",
    "# Adjust num_emg_features\n",
    "X_emg = emg_data_normalized[:, :, :num_emg_features]\n",
    "# Assuming the last time step represents the target\n",
    "y_emg = emg_data_normalized[:, -1, :]\n",
    "\n",
    "# Now you have X_kinematic, y_kinematic, X_emg, and y_emg for further processing\n",
    "\n",
    "# Split the data\n",
    "X_kinematic_train, X_kinematic_val, y_kinematic_train, y_kinematic_val = train_test_split(\n",
    "    X_kinematic, y_kinematic, test_size=0.3, random_state=42\n",
    ")\n",
    "X_emg_train, X_emg_val, y_emg_train, y_emg_val = train_test_split(\n",
    "    X_emg, y_emg, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Ensure shapes match the model input requirements\n",
    "# Add additional processing steps if necessary\n",
    "\n",
    "# Print the shapes for verification\n",
    "# print(\"Shapes of Kinematic Data Sets:\")\n",
    "# print(\"Train:\", X_train_kinematic.shape, y_train_kinematic.shape)\n",
    "# print(\"Validation:\", X_val_kinematic.shape, y_val_kinematic.shape)\n",
    "# print(\"Test:\", X_test_kinematic.shape, y_test_kinematic.shape)\n",
    "\n",
    "# print(\"\\nShapes of EMG Data Sets:\")\n",
    "# print(\"Train:\", X_train_emg.shape, y_train_emg.shape)\n",
    "# print(\"Validation:\", X_val_emg.shape, y_val_emg.shape)\n",
    "# print(\"Test:\", X_test_emg.shape, y_test_emg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model architecture</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Masking, RepeatVector, Layer, Reshape\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Assuming num_emg_features, num_kinematic_features, and num_joint_angle_features are defined\n",
    "num_emg_features = 7\n",
    "num_kinematic_features = 18\n",
    "\n",
    "# Total number of features\n",
    "num_input_features = num_emg_features + num_kinematic_features\n",
    "\n",
    "# Define the input sequence shape\n",
    "# Variable-length input sequence\n",
    "input_seq_shape = (None, num_input_features)\n",
    "\n",
    "# Define the GRU units\n",
    "gru_units = 32\n",
    "\n",
    "# EMG data branch\n",
    "encoder_inputs_emg = Input(shape=(None, num_emg_features), name='Input-Layer-EMG')\n",
    "encoder_emg = GRU(gru_units, return_sequences=True, name='Hidden-GRU-Encoder-Layer-EMG')(encoder_inputs_emg)\n",
    "\n",
    "# Kinematic data branch\n",
    "encoder_inputs_kinematic = Input(shape=(None, num_kinematic_features), name='Input-Layer-Kinematic')\n",
    "encoder_kinematic = GRU(gru_units, return_sequences=True, name='Hidden-GRU-Encoder-Layer-Kinematic')(encoder_inputs_kinematic)\n",
    "\n",
    "# Concatenate EMG and kinematic encodings\n",
    "encoder_combined = concatenate([encoder_emg, encoder_kinematic], axis=-1)\n",
    "\n",
    "# Attention Mechanism\n",
    "attention = Dot(axes=[1, 1], name='Attention-Layer')([encoder_combined, encoder_combined])\n",
    "attention = Activation('softmax', name='Attention-Activation')(attention)\n",
    "\n",
    "# Apply attention weights to encoder outputs\n",
    "context = Dot(axes=[1, 2], name='Context-Layer')([attention, encoder_combined])\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None, num_kinematic_features), name='Decoder-Input-Layer')\n",
    "decoder_gru = GRU(gru_units, return_sequences=True, name='Hidden-GRU-Decoder-Layer')(decoder_inputs)\n",
    "\n",
    "# Flatten context along with masking\n",
    "context_flattened = Flatten()(Masking()(context))\n",
    "\n",
    "# Custom layer to repeat the context along the time axis\n",
    "class RepeatContextLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RepeatContextLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.expand_dims(inputs, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 1, input_shape[1])\n",
    "\n",
    "context_expanded = RepeatContextLayer(name='Repeat-Context')(context_flattened)\n",
    "\n",
    "# Concatenate expanded context and decoder GRU output\n",
    "decoder_combined = Concatenate(axis=-1, name='Concatenate-Layer')([context_expanded, decoder_gru])\n",
    "\n",
    "# Flatten the decoder_combined while maintaining the sequence length\n",
    "decoder_flattened = Flatten(name='Flatten-Layer')(decoder_combined)\n",
    "\n",
    "print('Decoder shape before reshaping: ', decoder_flattened.shape)\n",
    "# Reshape to have a single dimension in the output\n",
    "decoder_output = Reshape((-1, 1), name='Reshape-Layer')(decoder_flattened)\n",
    "print('Flatten-Layer shape: ', decoder_output.shape)\n",
    "\n",
    "# Output layer\n",
    "outputs = TimeDistributed(Dense(1, activation='linear'), name='Output-Layer')(decoder_output)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[encoder_inputs_emg, encoder_inputs_kinematic, decoder_inputs], outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Print model summary for review\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST OF CODE IN COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming loaded_model_path is the path to your saved model file\n",
    "loaded_model_path = 'short_test_5_epoch.keras'\n",
    "\n",
    "# Load the model with custom_objects to recognize the GRU layer\n",
    "loaded_model = load_model(loaded_model_path)\n",
    "\n",
    "print(\"Model loaded successfully.\")\n",
    "print(loaded_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df_last_10_samples = df_all[df_all['Sample'] >= 89]\n",
    "test_eim = np.array(df_last_10_samples['EIMMagnitude'])\n",
    "print(test_eim)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "columns_to_normalize = ['EIMMagnitude', 'EIMPhase',\t'JointAngle', 'RollingAverageMag', 'RollingAveragePhase',\n",
    "                        'MedianEIMMagnitude', 'MedianEIMPhase', 'MedianJointAngle', 'MeanEIMMagnitude',\n",
    "                        'MeanEIMPhase', 'MeanJointAngle', 'StdEIMMagnitude', 'StdEIMPhase',\t'StdJointAngle',\n",
    "                        'VarEIMMagnitude', 'VarEIMPhase', 'VarJointAngle', 'KurtEIMMagnitude', 'KurtEIMPhase',\n",
    "                        'KurtJointAngle', 'ROCEIMMagnitude', 'ROCEIMPhase', 'ROCJointAngle']\n",
    "\n",
    "scaler.fit(df_last_10_samples[columns_to_normalize].values.reshape(-1,1))\n",
    "df_grouped = df_last_10_samples.groupby(['Sample'])\n",
    "\n",
    "# Split the dataset into input features (X) and target variables (y)\n",
    "num_features = 23\n",
    "# X = df_grouped[['EIMMagnitude', 'EIMPhase', 'RollingAverageMag', 'RollingAveragePhase','MedianEIMMagnitude',\n",
    "#         'MedianEIMPhase', 'MeanEIMMagnitude', 'MeanEIMPhase', 'StdEIMMagnitude', 'StdEIMPhase', 'VarEIMMagnitude',\n",
    "#         'VarEIMPhase', 'KurtEIMMagnitude', 'KurtEIMPhase', 'ROCEIMMagnitude', 'ROCEIMPhase']]\n",
    "# y = df_grouped[['JointAngle', 'Mass']]\n",
    "\n",
    "# Initialize scalers\n",
    "scalers_X = {}\n",
    "scalers_y = {}\n",
    "\n",
    "# Scale data and create sequences for each group\n",
    "X_seq, y_seq = [], []\n",
    "\n",
    "for group_name, group_data in df_grouped:\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    # Scale features\n",
    "    group_data_scaled = scaler.fit_transform(group_data[['JointAngle', 'MedianJointAngle', 'MeanJointAngle','StdJointAngle',\n",
    "                                                           'ROCJointAngle', 'VarJointAngle', 'KurtJointAngle',\n",
    "                                                           'EIMMagnitude', 'EIMPhase', 'RollingAverageMag', 'RollingAveragePhase',\n",
    "                                                           'MedianEIMMagnitude', 'MedianEIMPhase', 'MeanEIMMagnitude',\n",
    "                                                           'MeanEIMPhase', 'StdEIMMagnitude', 'StdEIMPhase', 'VarEIMMagnitude',\n",
    "                                                           'VarEIMPhase', 'KurtEIMMagnitude', 'KurtEIMPhase', 'ROCEIMMagnitude','ROCEIMPhase']])\n",
    "\n",
    "    # Scale target variables\n",
    "    group_data_scaled_y = scaler.fit_transform(group_data[['JointAngle', 'Mass']])\n",
    "\n",
    "    # Create sequences (adjust sequence_length as needed)\n",
    "    sequence_length = 10\n",
    "    for i in range(len(group_data) - sequence_length + 1):\n",
    "        X_seq.append(group_data_scaled[i:i+sequence_length, :])\n",
    "        y_seq.append(group_data_scaled_y[i+sequence_length-1, :])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "timestep = 10\n",
    "\n",
    "# X_seq, y_seqn = reshape_for_gru(df_last_10_samples, timestep, scaler, columns_to_use)\n",
    "\n",
    "predictions_scaled = loaded_model.predict(X_seq)\n",
    "predictions = scaler.inverse_transform(predictions_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
