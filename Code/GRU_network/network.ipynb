{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import GRU, Dense\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # This assumes you have your joint angles and EIM data in X and the corresponding labels (joint angle and weight) in y.\n",
    "\n",
    "# # Example data loading (replace this with your actual data loading code)\n",
    "# # X and y should be NumPy arrays\n",
    "# # X should have shape (num_samples, num_timesteps, num_features)\n",
    "# # y should have shape (num_samples, num_output_features)\n",
    "\n",
    "# # Generate example data (replace this with your actual data loading code)\n",
    "# np.random.seed(42)\n",
    "# num_samples = 1000\n",
    "# num_timesteps = 10\n",
    "# num_features = 2  # Replace with the actual number of features for joint angles and EIM data\n",
    "# num_output_features = 2  # Replace with the actual number of output features (joint angle and weight)\n",
    "\n",
    "# X = np.random.rand(num_samples, num_timesteps, num_features)\n",
    "# y = np.random.rand(num_samples, num_output_features)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Normalize the data using Min-Max scaling\n",
    "# scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "# X_train = scaler_x.fit_transform(X_train.reshape(-1, num_features)).reshape(X_train.shape)\n",
    "# X_test = scaler_x.transform(X_test.reshape(-1, num_features)).reshape(X_test.shape)\n",
    "\n",
    "# scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "# y_train = scaler_y.fit_transform(y_train)\n",
    "# y_test = scaler_y.transform(y_test)\n",
    "\n",
    "# # Build the GRU model\n",
    "# model = Sequential()\n",
    "# model.add(GRU(50, activation='relu', input_shape=(num_timesteps, num_features)))\n",
    "# model.add(Dense(num_output_features, activation='linear'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss = model.evaluate(X_test, y_test)\n",
    "# print(f'Test Loss: {loss}')\n",
    "\n",
    "# # Make predictions on new data\n",
    "# # Replace `new_data` with your actual new data\n",
    "# new_data = np.random.rand(1, num_timesteps, num_features)\n",
    "# scaled_new_data = scaler_x.transform(new_data.reshape(-1, num_features)).reshape(new_data.shape)\n",
    "# prediction = model.predict(scaled_new_data)\n",
    "# scaled_prediction = scaler_y.inverse_transform(prediction)\n",
    "# print(f'Predicted values: {scaled_prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Libaries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install pandas\n",
    "# %pip install scikit-learn\n",
    "# %pip install plotly\n",
    "# %pip install scipy\n",
    "\n",
    "# Tensorflow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "from keras.models import Sequential # for creating a linear stack of layers for our Neural Network\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Bidirectional, GRU, RepeatVector, Dense, TimeDistributed, concatenate, Dot, Activation, Concatenate, Flatten # for creating layers inside the Neural Network\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd # for data manipulation\n",
    "print('pandas: %s' % pd.__version__) # print version\n",
    "import numpy as np # for data manipulation\n",
    "print('numpy: %s' % np.__version__) # print version\n",
    "\n",
    "# Sklearn\n",
    "import sklearn\n",
    "print('sklearn: %s' % sklearn.__version__) # print version\n",
    "from sklearn.preprocessing import MinMaxScaler # for feature scaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "print('plotly: %s' % plotly.__version__) # print version\n",
    "\n",
    "import scipy.io\n",
    "print('scipy: %s' % scipy.__version__) # print version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas options to display more columns\n",
    "pd.options.display.max_columns=150\n",
    "\n",
    "# Google drive location weatherset: /content/drive/My Drive/Bachelor/NeuralNetwork/weatherAUS.csv\n",
    "# Local weatherset location: C:/Users/Simons Lenovo/Desktop/Neural_network_data/weatherAUS.csv\n",
    "data_dir = \"C:/Users/Simons Lenovo/Desktop/Neural_network_data/KIN_MUS_UJI.mat\"\n",
    "\n",
    "# Read in the weather data csv - keep only the columns we need\n",
    "# For csv read: df=pd.read_csv(data_dir, encoding='utf-8', usecols=['Date', 'Location', 'MinTemp', 'MaxTemp'])\n",
    "df=scipy.io.loadmat(data_dir)   # Specifically for loading .mat files\n",
    "\n",
    "# Access variables from the loaded data\n",
    "# Assuming the struct variable is named 'EMG_KIN_v4'\n",
    "data_struct = df['EMG_KIN_v4']\n",
    "\n",
    "# Drops records of NaN\n",
    "data_struct = data_struct[pd.isnull(data_struct['Kinematic_data'])==False]\n",
    "data_struct = data_struct[pd.isnull(data_struct['EMG_data'])==False]\n",
    "data_struct = data_struct[pd.isnull(data_struct['time'])==False]\n",
    "\n",
    "# Access individual variables from the struct\n",
    "kinematic_data = data_struct['Kinematic_data']\n",
    "emg_data = data_struct['EMG_data']\n",
    "time = data_struct['time']\n",
    "\n",
    "# Manipulate data as needed for your GRU network\n",
    "# For example, you might want to reshape time for compatibility with GRU\n",
    "time_reshaped = time.reshape(-1, 1)  # Assuming 1D time array\n",
    "\n",
    "# You can now use kinematic_data and emg_data as your features (X) and time as needed\n",
    "# Ensure that the shapes and dimensions are appropriate for your model\n",
    "\n",
    "# Print example values\n",
    "print('Example Kinematic Data:', kinematic_data[0])\n",
    "print('Example EMG Data:', emg_data[0])\n",
    "print('Example Time:', time_reshaped[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data preprocessing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing. Range of the scaler should be the same for x and y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have features X and time\n",
    "# Normalize the data\n",
    "scaler_kinematic = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_emg = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Flatten and concatenate all samples for kinematic_data and emg_data\n",
    "kinematic_data_flat = np.concatenate([sample.flatten() for sample in kinematic_data])\n",
    "emg_data_flat = np.concatenate([sample.flatten() for sample in emg_data])\n",
    "\n",
    "print('Before normalization - Kinematic Data Shape:', kinematic_data.shape)\n",
    "print('Before normalization - EMG Data Shape:', emg_data.shape)\n",
    "print('Flattened Shapes - Kinematic:', kinematic_data_flat.shape, 'EMG:', emg_data_flat.shape)\n",
    "\n",
    "# Normalize entire datasets\n",
    "kinematic_data_normalized = scaler_kinematic.fit_transform(kinematic_data_flat.reshape(-1, 1))\n",
    "emg_data_normalized = scaler_emg.fit_transform(emg_data_flat.reshape(-1, 1))\n",
    "\n",
    "print('After normalization - Kinematic Data Normalized Shape:', kinematic_data_normalized.shape)\n",
    "print('After normalization - EMG Data Normalized Shape:', emg_data_normalized.shape)\n",
    "\n",
    "# Reshape back to the original shape - Not necesary for GRU\n",
    "# kinematic_data_normalized = kinematic_data_normalized.reshape(kinematic_data.shape)\n",
    "# emg_data_normalized = emg_data_normalized.reshape(emg_data.shape)\n",
    "\n",
    "# print('After reshaping - Kinematic Data Normalized Shape:', kinematic_data_normalized.shape)\n",
    "# print('After reshaping - EMG Data Normalized Shape:', emg_data_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prepare sequences for GRU</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important for synchronizing the data with eachother with a common time variable. The \"time-variable\" from the dataset is therefor strictly speaking not needed for training the network.\n",
    "\n",
    "The time_steps variable is a hyper parameter, that determines how many prior time steps the network should take into account. If time_steps=10 for example, each input sequence will contain data from the previous 10 steps, and the model will try to learn patterns based on this history.\n",
    "\n",
    "Using a smaller number of time steps might help the model capture short-term dependencies, while a larger number could capture longer-term patterns. It's essential to experiment and choose a value that best fits the nature of the data and the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, time_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:(i + time_steps)])\n",
    "        y.append(data[i + time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set the number of time steps (adjust as needed). \n",
    "time_steps = 10\n",
    "\n",
    "# Create sequences for kinematic data (assuming it's univariate)\n",
    "X_kinematic, y_kinematic = create_sequences(kinematic_data_normalized, time_steps)\n",
    "\n",
    "# Create sequences for EMG data if needed (assuming it's univariate)\n",
    "X_emg, y_emg = create_sequences(emg_data_normalized, time_steps)\n",
    "\n",
    "print('X_kinematic: ', X_kinematic.shape)\n",
    "print('y_kinematic: ', y_kinematic.shape)\n",
    "print('X_emg: ', X_emg.shape)\n",
    "print('y_emg: ', y_emg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Splitting the data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the data into a training/evaluation/test distribution of 70/20/10. The random_state parameter is a seed for the random split, that allows reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the kinematic data into training, validation, and test sets\n",
    "X_train_kinematic, X_temp_kinematic, y_train_kinematic, y_temp_kinematic = train_test_split(\n",
    "    X_kinematic, y_kinematic, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Further split the temp sets into validation and test sets\n",
    "X_val_kinematic, X_test_kinematic, y_val_kinematic, y_test_kinematic = train_test_split(\n",
    "    X_temp_kinematic, y_temp_kinematic, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# Repeat the same for EMG data\n",
    "X_train_emg, X_temp_emg, y_train_emg, y_temp_emg = train_test_split(\n",
    "    X_emg, y_emg, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "X_val_emg, X_test_emg, y_val_emg, y_test_emg = train_test_split(\n",
    "    X_temp_emg, y_temp_emg, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# Ensure shapes match the model input requirements\n",
    "# Add additional processing steps if necessary\n",
    "\n",
    "# Print the shapes for verification\n",
    "print(\"Shapes of Kinematic Data Sets:\")\n",
    "print(\"Train:\", X_train_kinematic.shape, y_train_kinematic.shape)\n",
    "print(\"Validation:\", X_val_kinematic.shape, y_val_kinematic.shape)\n",
    "print(\"Test:\", X_test_kinematic.shape, y_test_kinematic.shape)\n",
    "\n",
    "print(\"\\nShapes of EMG Data Sets:\")\n",
    "print(\"Train:\", X_train_emg.shape, y_train_emg.shape)\n",
    "print(\"Validation:\", X_val_emg.shape, y_val_emg.shape)\n",
    "print(\"Test:\", X_test_emg.shape, y_test_emg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model architecture</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flexibility:** Using a Seq2Seq (sequence to sequence) architecture with GRU as RNN's, since the length of the EIM signals might vary, and Seq2Seq can handle this. Normal GRU would require to truncate the samples to the same lengths. The Seq2Seq model provides more flexibility in capturing complex relationships between variable-length sequences, making it potentially more suitable for tasks where the alignment between input and output elements is not fixed.\n",
    "\n",
    "**Attention mechanism:** The attention mechanism in the Seq2Seq model allows the network to focus on different parts of the input sequence when making predictions for each element in the output sequence. This is beneficial when the relationship between EMG signals and joint angles might not be straightforward and might require the model to attend to different segments of the input sequence.\n",
    "\n",
    "Splits the EMG and kinematic data into two sepparate branches to train on. The idea is to create a model that can learn the relationship between EMG data and joint angles by processing both types of data during training.\n",
    "\n",
    "During the training phase, the model learns to capture patterns and relationships between the provided EMG signals and the corresponding joint angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Masking, RepeatVector, Layer, Reshape\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Assuming num_emg_features, num_kinematic_features, and num_joint_angle_features are defined\n",
    "num_emg_features = 7\n",
    "num_kinematic_features = 18\n",
    "\n",
    "# Total number of features\n",
    "num_input_features = num_emg_features + num_kinematic_features\n",
    "\n",
    "# Define the input sequence shape\n",
    "input_seq_shape = (None, num_input_features)  # Variable-length input sequence\n",
    "\n",
    "# Define the GRU units\n",
    "gru_units = 32\n",
    "\n",
    "# EMG data branch\n",
    "encoder_inputs_emg = Input(shape=(None, num_emg_features), name='Input-Layer-EMG')\n",
    "encoder_emg = GRU(gru_units, return_sequences=True, name='Hidden-GRU-Encoder-Layer-EMG')(encoder_inputs_emg)\n",
    "\n",
    "# Kinematic data branch\n",
    "encoder_inputs_kinematic = Input(shape=(None, num_kinematic_features), name='Input-Layer-Kinematic')\n",
    "encoder_kinematic = GRU(gru_units, return_sequences=True, name='Hidden-GRU-Encoder-Layer-Kinematic')(encoder_inputs_kinematic)\n",
    "\n",
    "# Concatenate EMG and kinematic encodings\n",
    "encoder_combined = concatenate([encoder_emg, encoder_kinematic], axis=-1)\n",
    "\n",
    "# Attention Mechanism\n",
    "attention = Dot(axes=[1, 1], name='Attention-Layer')([encoder_combined, encoder_combined])\n",
    "attention = Activation('softmax', name='Attention-Activation')(attention)\n",
    "\n",
    "# Apply attention weights to encoder outputs\n",
    "context = Dot(axes=[1, 2], name='Context-Layer')([attention, encoder_combined])\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None, num_kinematic_features), name='Decoder-Input-Layer')\n",
    "decoder_gru = GRU(gru_units, return_sequences=True, name='Hidden-GRU-Decoder-Layer')(decoder_inputs)\n",
    "\n",
    "# Flatten context along with masking\n",
    "context_flattened = Flatten()(Masking()(context))\n",
    "\n",
    "# Custom layer to repeat the context along the time axis\n",
    "class RepeatContextLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RepeatContextLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.expand_dims(inputs, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 1, input_shape[1])\n",
    "\n",
    "context_expanded = RepeatContextLayer(name='Repeat-Context')(context_flattened)\n",
    "\n",
    "# Concatenate expanded context and decoder GRU output\n",
    "decoder_combined = Concatenate(axis=-1, name='Concatenate-Layer')([context_expanded, decoder_gru])\n",
    "\n",
    "# Flatten the decoder_combined while maintaining the sequence length\n",
    "decoder_flattened = Flatten(name='Flatten-Layer')(decoder_combined)\n",
    "\n",
    "print('Decoder shape before reshaping: ', decoder_flattened.shape)\n",
    "# Reshape to have a single dimension in the output\n",
    "decoder_output = Reshape((-1, 1), name='Reshape-Layer')(decoder_flattened)\n",
    "print('Flatten-Layer shape: ', decoder_output.shape)\n",
    "\n",
    "# Output layer\n",
    "outputs = TimeDistributed(Dense(1, activation='linear'), name='Output-Layer')(decoder_output)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[encoder_inputs_emg, encoder_inputs_kinematic, decoder_inputs], outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Print model summary for review\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# model.fit(X_train_kinematic, y_train_kinematic, epochs=10, batch_size=32, validation_data=(X_test_kinematic, y_test_kinematic))\n",
    "\n",
    "# Use EarlyStopping to stop training when the validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model on both EMG and kinematic data\n",
    "# Set the number of epochs and batch size\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Training the model\n",
    "model.fit(\n",
    "    [X_train_emg, X_train_kinematic],  # Only EMG and kinematic inputs during training\n",
    "    [y_train_kinematic, y_train_kinematic[:, 1:]],  # y_train_kinematic[:, 1:] is the shifted version for training the decoder,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=([X_val_emg, X_val_kinematic], y_val_kinematic),  # Adjust for validation data\n",
    "    callbacks=[early_stopping]  # If you are using EarlyStopping\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate([X_test_emg, X_test_kinematic], y_test_kinematic)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three months are missing from the dataset, so average from preceding and subsequent months are calculated as placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add missing months 2011-04, 2011-04, 2011-04 and impute data\n",
    "df2_pivot['2011-04']=(df2_pivot['2011-03']+df2_pivot['2011-05'])/2\n",
    "df2_pivot['2012-12']=(df2_pivot['2012-11']+df2_pivot['2013-01'])/2\n",
    "df2_pivot['2013-02']=(df2_pivot['2013-01']+df2_pivot['2013-03'])/2\n",
    "\n",
    "# Sort columns so Year-Months are in the correct order\n",
    "df2_pivot=df2_pivot.reindex(sorted(df2_pivot.columns), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nbformat\n",
    "\n",
    "# Plot average monthly temperature derived from daily medians for each location\n",
    "fig = go.Figure()\n",
    "for location in df2_pivot.index:\n",
    "    fig.add_trace(go.Scatter(x=df2_pivot.loc[location, :].index,\n",
    "                             y=df2_pivot.loc[location, :].values,\n",
    "                             mode='lines',\n",
    "                             name=location,\n",
    "                             opacity=0.8,\n",
    "                             line=dict(width=1)\n",
    "                            ))\n",
    "\n",
    "# Change chart background color\n",
    "fig.update_layout(dict(plot_bgcolor = 'white'), showlegend=True)\n",
    "\n",
    "# Update axes lines\n",
    "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "                 showline=True, linewidth=1, linecolor='black',\n",
    "                 title='Date'\n",
    "                )\n",
    "\n",
    "fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "                 showline=True, linewidth=1, linecolor='black',\n",
    "                 title='Degrees Celsius'\n",
    "                )\n",
    "\n",
    "# Set figure title\n",
    "fig.update_layout(title=dict(text=\"Average Monthly Temperatures\", font=dict(color='black')))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the mean temperature, as well as variation, differs between locations. We can either train a location-specific model for better precision or a generic model to predict temperatures for every area.\n",
    "\n",
    "In this example, I will create a generic model trained on all locations.\n",
    "\n",
    "Training and evaluating GRU model Here are a few things to highlight before we start.\n",
    "\n",
    "We will use sequences of 18 months to predict the average temperatures for the next 18 months. You can adjust that to your liking but beware that there will not be enough data for sequences beyond 23 months in length.\n",
    "\n",
    "We will split the data into two separate dataframes — one for training and the other for validation (out of time validation).\n",
    "\n",
    "Since we are creating a many-to-many prediction model, we need to use a slightly more complex encoder-decoder configuration. Both encoder and decoder are hidden GRU layers, with information passed from one to another via a repeat vector layer.\n",
    "\n",
    "A repeat vector is necessary when we want to have sequences of different lengths, e.g., a sequence of 18 months to predict the next 12 months. It ensures that we provide the right shape for a decoder layer. However, if your input and output sequences are of the same length as in my example, then you can also choose to set return_sequences=True in the encoder layer and remove the repeat vector. Note that we added a Bidirectional wrapper to GRU layers. It allows us to train the model in both directions, which sometimes produces better results. However, its use is optional.\n",
    "\n",
    "Also, we need to use a Time Distributed wrapper in the output layer to predict outputs for each timestep individually. Finally, I have used MinMaxScaling in this example because it has produced better results than the unscaled version. You can find both scaled and unscaled setups within Jupyter Notebooks in my GitHub repository (link available at the end of the article).\n",
    "\n",
    "First, let’s define a helper function to reshape the data to a 3D array required by GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shaping(datain, timestep, scaler):\n",
    "\n",
    "    # Loop through each location\n",
    "    for location in datain.index:\n",
    "        datatmp = datain[datain.index==location].copy()\n",
    "\n",
    "        # Convert input dataframe to array and flatten\n",
    "        arr=datatmp.to_numpy().flatten()\n",
    "\n",
    "        # Scale using transform (using previously fitted scaler)\n",
    "        arr_scaled=scaler.transform(arr.reshape(-1, 1)).flatten()\n",
    "\n",
    "        cnt=0\n",
    "        for mth in range(0, len(datatmp.columns)-(2*timestep)+1): # Define range\n",
    "            cnt=cnt+1 # Gives us the number of samples. Later used to reshape the data\n",
    "            X_start=mth # Start month for inputs of each sample\n",
    "            X_end=mth+timestep # End month for inputs of each sample\n",
    "            Y_start=mth+timestep # Start month for targets of each sample. Note, start is inclusive and end is exclusive, that's why X_end and Y_start is the same number\n",
    "            Y_end=mth+2*timestep # End month for targets of each sample.\n",
    "\n",
    "            # Assemble input and target arrays containing all samples\n",
    "            if mth==0:\n",
    "                X_comb=arr_scaled[X_start:X_end]\n",
    "                Y_comb=arr_scaled[Y_start:Y_end]\n",
    "            else:\n",
    "                X_comb=np.append(X_comb, arr_scaled[X_start:X_end])\n",
    "                Y_comb=np.append(Y_comb, arr_scaled[Y_start:Y_end])\n",
    "\n",
    "        # Reshape input and target arrays\n",
    "        X_loc=np.reshape(X_comb, (cnt, timestep, 1))\n",
    "        Y_loc=np.reshape(Y_comb, (cnt, timestep, 1))\n",
    "\n",
    "        # Append an array for each location to the master array\n",
    "        if location==datain.index[0]:\n",
    "            X_out=X_loc\n",
    "            Y_out=Y_loc\n",
    "        else:\n",
    "            X_out=np.concatenate((X_out, X_loc), axis=0)\n",
    "            Y_out=np.concatenate((Y_out, Y_loc), axis=0)\n",
    "\n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train GRU neural network over 50 epochs and display the model summary with evaluation metrics. You can follow my comments within the code to understand each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 1 - Specify parameters\n",
    "timestep=18\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 2 - Prepare data\n",
    "\n",
    "# Split data into train and test dataframes\n",
    "df_train=df2_pivot.iloc[:, 0:-2*timestep].copy()\n",
    "df_test=df2_pivot.iloc[:, -2*timestep:].copy()\n",
    "\n",
    "# Use fit to train the scaler on the training data only, actual scaling will be done inside reshaping function\n",
    "scaler.fit(df_train.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Use previously defined shaping function to reshape the data for GRU\n",
    "X_train, Y_train = shaping(datain=df_train, timestep=timestep, scaler=scaler)\n",
    "X_test, Y_test = shaping(datain=df_test, timestep=timestep, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 3 - Specify the structure of a Neural Network\n",
    "model = Sequential(name=\"GRU-Model\") # Model\n",
    "model.add(Input(shape=(X_train.shape[1],X_train.shape[2]), name='Input-Layer')) # Input Layer - need to speicfy the shape of inputs\n",
    "model.add(Bidirectional(GRU(units=32, activation='tanh', recurrent_activation='sigmoid', stateful=False), name='Hidden-GRU-Encoder-Layer')) # Encoder Layer\n",
    "model.add(RepeatVector(X_train.shape[1], name='Repeat-Vector-Layer')) # Repeat Vector\n",
    "model.add(Bidirectional(GRU(units=32, activation='tanh', recurrent_activation='sigmoid', stateful=False, return_sequences=True), name='Hidden-GRU-Decoder-Layer')) # Decoder Layer\n",
    "model.add(TimeDistributed(Dense(units=1, activation='linear'), name='Output-Layer')) # Output Layer, Linear(x) = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 4 - Compile the model\n",
    "model.compile(optimizer='adam', # default='rmsprop', an algorithm to be used in backpropagation\n",
    "              loss='mean_squared_error', # Loss function to be optimized. A string (name of loss function), or a tf.keras.losses.Loss instance.\n",
    "              metrics=['MeanSquaredError', 'MeanAbsoluteError'], # List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance.\n",
    "              loss_weights=None, # default=None, Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs.\n",
    "              weighted_metrics=None, # default=None, List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing.\n",
    "              run_eagerly=None, # Defaults to False. If True, this Model's logic will not be wrapped in a tf.function. Recommended to leave this as None unless your Model cannot be run inside a tf.function.\n",
    "              steps_per_execution=None # Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead.\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 5 - Fit the model on the dataset\n",
    "history = model.fit(X_train, # input data\n",
    "                    Y_train, # target data\n",
    "                    batch_size=1, # Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
    "                    epochs=50, # default=1, Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided\n",
    "                    verbose=1, # default='auto', ('auto', 0, 1, or 2). Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy.\n",
    "                    callbacks=None, # default=None, list of callbacks to apply during training. See tf.keras.callbacks\n",
    "                    validation_split=0.2, # default=0.0, Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch.\n",
    "                    #validation_data=(X_test, y_test), # default=None, Data on which to evaluate the loss and any model metrics at the end of each epoch.\n",
    "                    shuffle=True, # default=True, Boolean (whether to shuffle the training data before each epoch) or str (for 'batch').\n",
    "                    class_weight=None, # default=None, Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n",
    "                    sample_weight=None, # default=None, Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only).\n",
    "                    initial_epoch=0, # Integer, default=0, Epoch at which to start training (useful for resuming a previous training run).\n",
    "                    steps_per_epoch=None, # Integer or None, default=None, Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined.\n",
    "                    validation_steps=None, # Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch.\n",
    "                    validation_batch_size=None, # Integer or None, default=None, Number of samples per validation batch. If unspecified, will default to batch_size.\n",
    "                    validation_freq=10, # default=1, Only relevant if validation data is provided. If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs.\n",
    "                    max_queue_size=10, # default=10, Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10.\n",
    "                    workers=1, # default=1, Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1.\n",
    "                    use_multiprocessing=True, # default=False, Used for generator or keras.utils.Sequence input only. If True, use process-based threading. If unspecified, use_multiprocessing will default to False.\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 6 - Use model to make predictions\n",
    "# Predict results on training data\n",
    "#pred_train = model.predict(X_train)\n",
    "# Predict results on test data\n",
    "pred_test = model.predict(X_test)\n",
    "\n",
    "\n",
    "##### Step 7 - Print Performance Summary\n",
    "print(\"\")\n",
    "print('-------------------- Model Summary --------------------')\n",
    "model.summary() # print model summary\n",
    "print(\"\")\n",
    "print('-------------------- Weights and Biases --------------------')\n",
    "print(\"Too many parameters to print but you can use the code provided if needed\")\n",
    "print(\"\")\n",
    "#for layer in model.layers:\n",
    "#    print(layer.name)\n",
    "#    for item in layer.get_weights():\n",
    "#        print(\"  \", item)\n",
    "#print(\"\")\n",
    "\n",
    "# Print the last value in the evaluation metrics contained within history file\n",
    "print('-------------------- Evaluation on Training Data --------------------')\n",
    "for item in history.history:\n",
    "    print(\"Final\", item, \":\", history.history[item][-1])\n",
    "print(\"\")\n",
    "\n",
    "# Evaluate the model on the test data using \"evaluate\"\n",
    "print('-------------------- Evaluation on Test Data --------------------')\n",
    "results = model.evaluate(X_test, Y_test)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s regenerate predictions for the 5 locations we picked earlier and plot the results on a chart to compare actual and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select locations to predict temperatures for\n",
    "location=['Cairns', 'Canberra', 'Darwin', 'GoldCoast', 'MountGinini']\n",
    "dfloc_test = df_test[df_test.index.isin(location)].copy()\n",
    "\n",
    "# Reshape test data\n",
    "X_test, Y_test = shaping(datain=dfloc_test, timestep=timestep, scaler=scaler)\n",
    "\n",
    "# Predict results on test data\n",
    "pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average monthly temperatures (actual and predicted) for test (out of time) data\n",
    "fig = go.Figure()\n",
    "\n",
    "# Trace for actual temperatures\n",
    "for location in dfloc_test.index:\n",
    "    fig.add_trace(go.Scatter(x=dfloc_test.loc[location, :].index,\n",
    "                             y=dfloc_test.loc[location, :].values,\n",
    "                             mode='lines',\n",
    "                             name=location,\n",
    "                             opacity=0.8,\n",
    "                             line=dict(width=1)\n",
    "                            ))\n",
    "\n",
    "# Trace for predicted temperatures\n",
    "for i in range(0,pred_test.shape[0]):\n",
    "    fig.add_trace(go.Scatter(x=np.array(dfloc_test.columns[-timestep:]),\n",
    "                             # Need to inverse transform the predictions before plotting\n",
    "                             y=scaler.inverse_transform(pred_test[i].reshape(-1,1)).flatten(),\n",
    "                             mode='lines',\n",
    "                             name=dfloc_test.index[i]+' Prediction',\n",
    "                             opacity=1,\n",
    "                             line=dict(width=2, dash='dot')\n",
    "                            ))\n",
    "\n",
    "# Change chart background color\n",
    "fig.update_layout(dict(plot_bgcolor = 'white'))\n",
    "\n",
    "# Update axes lines\n",
    "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "                 showline=True, linewidth=1, linecolor='black',\n",
    "                 title='Year-Month'\n",
    "                )\n",
    "\n",
    "fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "                 showline=True, linewidth=1, linecolor='black',\n",
    "                 title='Degrees Celsius'\n",
    "                )\n",
    "\n",
    "# Set figure title\n",
    "fig.update_layout(title=dict(text=\"Average Monthly Temperatures\", font=dict(color='black')))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
