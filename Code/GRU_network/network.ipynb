{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Libaries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install pandas\n",
    "# %pip install scikit-learn\n",
    "# %pip install plotly\n",
    "# %pip install scipy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "from keras.models import Sequential\n",
    "from keras import Input\n",
    "from keras.layers import Bidirectional, GRU, RepeatVector, Dense, TimeDistributed, concatenate, Dot, Activation, Concatenate, Flatten # for creating layers inside the Neural Network\n",
    "\n",
    "import pandas as pd\n",
    "print('pandas: %s' % pd.__version__)\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "\n",
    "import sklearn\n",
    "print('sklearn: %s' % sklearn.__version__)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "print('plotly: %s' % plotly.__version__)\n",
    "\n",
    "import scipy.io\n",
    "from scipy.interpolate import interp1d\n",
    "print('scipy: %s' % scipy.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>From several csv files</h3>\n",
    "This section synchronizes and concatinates the EIM and kinematic data through unix time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_mean(input_signal):\n",
    "    output_signal = input_signal.copy()\n",
    "    buffer = len(input_signal) // 50\n",
    "    running_sum = 0.0\n",
    "\n",
    "    for i in range(len(input_signal)):\n",
    "        running_sum += input_signal[i]\n",
    "\n",
    "        if i < buffer:\n",
    "            output_signal[i] = running_sum / float(i + 1)\n",
    "        else:\n",
    "            running_sum -= input_signal[i - buffer]\n",
    "            output_signal[i] = running_sum / float(buffer)\n",
    "\n",
    "    return output_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas options to display more columns\n",
    "pd.options.display.max_columns=150\n",
    "\n",
    "# Loading data and adding headers 'ElbowAngles' and 'Time'\n",
    "kinematic_data_dir = 'MoCap/dynamic/99_HRML_4kg_pd/elbow_angles.csv'\n",
    "eim_data_dir = 'MoCap/dynamic/99_HRML_4kg_pd/processed_output_data.csv'\n",
    "df_kin=pd.read_csv(kinematic_data_dir, encoding='utf-8', names=[\"ElbowAngles\", \"Time\"])\n",
    "df_eim=pd.read_csv(eim_data_dir, encoding='utf-8')\n",
    "\n",
    "# Stripping data of unwanted delimiters and converting to float\n",
    "df_kin['ElbowAngles'] = df_kin['ElbowAngles'].str.strip('[]').astype(float)\n",
    "df_kin['Time'] = df_kin['Time'].str.strip('[]').astype(str)\n",
    "df_kin['Time'] = df_kin['Time'].str.strip('\\'').astype(float)\n",
    "\n",
    "# Extracting Unix time\n",
    "kin_unix = df_kin['Time'].values\n",
    "eim_unix = df_eim['Time'].values\n",
    "\n",
    "# Extracting max and min Unix values of kin and using these to figure out where to slice the EIM data\n",
    "kin_min = kin_unix.min()\n",
    "kin_max = kin_unix.max()\n",
    "\n",
    "# Filter EIM data based on kinematic Unix time range\n",
    "df_eim = df_eim[(df_eim['Time'] >= kin_min) & (df_eim['Time'] <= kin_max)]\n",
    "\n",
    "# Creating timestamps. EIM samples at 1000hZ, so 1 timestamp will correspond to 1ms\n",
    "df_eim['Timestamp'] = np.linspace(0, (len(df_eim) - 1), len(df_eim))\n",
    "df_kin['Timestamp'] = np.linspace(0, (len(df_kin) - 1), len(df_kin))\n",
    "\n",
    "# Extract timestamps and kinematic data\n",
    "kinematic_timestamps = df_kin['Timestamp'].values\n",
    "kinematic_data = df_kin['ElbowAngles'].values\n",
    "eim_timestamps = df_eim['Timestamp'].values\n",
    "\n",
    "# Interpolation of the kinematic data to match the EIM data\n",
    "shape = eim_timestamps.shape\n",
    "kinematic_interpolated = np.empty(shape)\n",
    "kin_idx = 0\n",
    "step = math.floor(len(eim_timestamps)/len(kinematic_timestamps))\n",
    "step_remainder = len(eim_timestamps)/len(kinematic_timestamps) - step\n",
    "step_temp = 0\n",
    "temp = 0\n",
    "\n",
    "# Linear interpolation done by averaging between two points. Each data step is done based \n",
    "# on the integer difference between the lenghts of the datasets. For increased accuracy, \n",
    "# whenever the remainder of the division becomes equal to or greater than 1, 1 is added \n",
    "# to the step, and withdrawn from the counter.\n",
    "for i in range(0, len(kinematic_data), 1):\n",
    "    kinematic_interpolated[kin_idx] = kinematic_data[i]\n",
    "    if i < len(kinematic_data)-1:\n",
    "        temp = kinematic_data[i+1]\n",
    "        for j in range(kin_idx + 1, kin_idx+step, 1):\n",
    "            kinematic_interpolated[j] = (kinematic_interpolated[j-1] + temp) / 2\n",
    "    else:\n",
    "        temp = kinematic_data[i]\n",
    "        for j in range(kin_idx + 1, len(eim_timestamps), 1):\n",
    "            kinematic_interpolated[j] = (kinematic_interpolated[j-1] + temp) / 2\n",
    "\n",
    "        step_temp = step_temp+step_remainder\n",
    "    if step_temp >= 1:\n",
    "        kin_idx = kin_idx + step + 1\n",
    "        step_temp = step_temp - 1\n",
    "    else:\n",
    "        kin_idx = kin_idx + step\n",
    "\n",
    "\n",
    "# Interpolate kinematic data to match EIM timestamps. Lines are drawn between the spread out data. \n",
    "# Missing values are being extrapolated\n",
    "kinematic_interpolated = interp1d(eim_timestamps, kinematic_interpolated, kind='linear', fill_value='extrapolate')\n",
    "\n",
    "# The interpolated data is being saved to the JointAngle column in the eim_data\n",
    "df_eim['JointAngle'] = kinematic_interpolated(eim_timestamps)\n",
    "\n",
    "# df_eim['RollingAverageMag'] = df_eim['EIMMagnitude'].rolling(100).mean()\n",
    "# df_eim['RollingAveragePhase'] = df_eim['EIMPhase'].rolling(100).mean()\n",
    "\n",
    "# Calculating rolling mean\n",
    "\n",
    "df_eim['RollingAverageMag'] = rolling_mean(df_eim['EIMMagnitude'])\n",
    "df_eim['RollingAveragePhase'] = rolling_mean(df_eim['EIMPhase'])\n",
    "df_eim['RollingStdEIM'] = df_eim['EIMMagnitude'].rolling(100).std()\n",
    "std_value=df_eim['RollingStdEIM'][99]\n",
    "df_eim['RollingStdEIM'].fillna(value=std_value, inplace=True) \n",
    "\n",
    "#Filling NaN values out with the first mean value in the series\n",
    "# mean_value=df_eim['RollingAverageMag'][99]\n",
    "# df_eim['RollingAverageMag'].fillna(value=mean_value, inplace=True) \n",
    "# mean_value=df_eim['RollingAveragePhase'][99]\n",
    "# df_eim['RollingAveragePhase'].fillna(value=mean_value, inplace=True) \n",
    "\n",
    "\n",
    "# NB! THIS SECTION OF SAVING THE FILES INTO ONE CSV FILE HAS BEEN COMMENTED OUT, NOT \n",
    "# TO RISK OVERWRITING THE all_samples.csv FILE\n",
    "\n",
    "# Saving the result to csv, where all samples are gonna be saved\n",
    "# df_final = df_eim[['Sample', 'EIMMagnitude', 'EIMPhase', 'JointAngle', 'Mass', 'Time', 'RollingAverageMag', 'RollingAveragePhase']]\n",
    "# df_eim_kin = 'all_samples.csv'\n",
    "# \n",
    "# if(os.path.isfile(df_eim_kin)):\n",
    "#     sample_number = input(\"Please provide the sample number and press enter:\")\n",
    "#     mass = input(\"Please provide the mass used in the current sample:\")\n",
    "#     df_final = df_final.assign(Sample=sample_number)\n",
    "#     df_final = df_final.assign(Mass=mass)\n",
    "#     # df_final.loc[:]['Sample'] = sample_number\n",
    "#     # df_final.loc[:]['Mass'] = mass\n",
    "#     # df_final['Sample'] = df_final['Sample'].replace(df_final['Sample'], sample_number)\n",
    "#     # df_final['Mass'] = df_final['Mass'].replace(df_final['Mass'], sample_number)\n",
    "\n",
    "#     df_final.to_csv(df_eim_kin, mode='a', index= False, header=False)\n",
    "# else:\n",
    "#     sample_number = input(\"Please provide the sample number and press enter:\")\n",
    "#     mass = input(\"Please provide the mass used in the current sample:\")\n",
    "#     df_final = df_final.assign(Sample=sample_number)\n",
    "#     df_final = df_final.assign(Mass=mass)\n",
    "#     # df_final['Sample'] = df_final['Sample'].replace(df_final['Sample'], sample_number)\n",
    "#     # df_final['Mass'] = df_final['Mass'].replace(df_final['Mass'], sample_number)\n",
    "\n",
    "#     df_final.to_csv(df_eim_kin, mode='w', index= False)\n",
    "\n",
    "# Assuming df_eim is your DataFrame\n",
    "plt.plot(df_eim['Timestamp'], df_eim['JointAngle'], marker='o')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('ElbowAngles')\n",
    "plt.title('Line Plot of ElbowAngles')\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(df_eim['Timestamp'], df_eim['EIMMagnitude'], marker='o')\n",
    "# plt.xlabel('Timestamp')\n",
    "# plt.ylabel('EIMMagnitude')\n",
    "# plt.title('Line Plot of EIMMagnitude')\n",
    "# plt.show()\n",
    "\n",
    "plt.plot(df_eim['Timestamp'], df_eim['RollingAverageMag'], marker='o')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('RollingAverageEIM')\n",
    "plt.title('Line Plot of RollingAverageEIM')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(df_eim['Timestamp'], df_eim['RollingStdEIM'], marker='o')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('RollingStdEIM')\n",
    "plt.title('Line Plot of RollingStdEIM')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(df_eim['Timestamp'], df_eim['RollingAveragePhase'], marker='o')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('EIMPhase')\n",
    "plt.title('Line Plot of EIMPhase')\n",
    "plt.show()\n",
    "\n",
    "# print(df_final.head(100))\n",
    "print(df_eim.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>From one csv file</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load csv of all samples. Group data by 'Sample'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_all = 'all_samples.csv'\n",
    "df_all=pd.read_csv(dir_all, encoding='utf-8')\n",
    "grouped = df_all.groupby('Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas options to display more columns\n",
    "pd.options.display.max_columns=150\n",
    "\n",
    "# Calculate the median for each group\n",
    "median_values = grouped[['EIMMagnitude', 'EIMPhase']].apply(lambda group: group[['EIMMagnitude', 'EIMPhase']].agg(['min', 'max']).median())\n",
    "\n",
    "# Calculate the mean for each group\n",
    "mean_values = grouped[['EIMMagnitude', 'EIMPhase']].mean()\n",
    "\n",
    "# Calculate the standard deviation for each group\n",
    "standard_deviations = grouped[['EIMMagnitude', 'EIMPhase']].std()\n",
    "\n",
    "# Calculate the variance for each group\n",
    "variance_values = grouped[['EIMMagnitude', 'EIMPhase']].var()\n",
    "\n",
    "# Calculate the kurtosis for each group\n",
    "kurtosis_values = grouped[['EIMMagnitude', 'EIMPhase']].apply(pd.DataFrame.kurtosis)\n",
    "\n",
    "# Reset the index to get the 'Sample' column back\n",
    "median_values.reset_index(inplace=True)\n",
    "mean_values.reset_index(inplace=True)\n",
    "standard_deviations.reset_index(inplace=True)\n",
    "variance_values.reset_index(inplace=True)\n",
    "kurtosis_values.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns to indicate they represent the median\n",
    "median_values.columns = ['Sample', 'MedianEIMMagnitude', 'MedianEIMPhase']\n",
    "mean_values.columns = ['Sample', 'MeanEIMMagnitude', 'MeanEIMPhase']\n",
    "standard_deviations.columns = ['Sample', 'StdEIMMagnitude', 'StdEIMPhase']\n",
    "variance_values.columns = ['Sample', 'VarEIMMagnitude', 'VarEIMPhase']\n",
    "kurtosis_values.columns = ['Sample', 'KurtEIMMagnitude', 'KurtEIMPhase']\n",
    "\n",
    "# Merge the median and mean values back into the original DataFrame based on the 'Sample' column\n",
    "df_all = df_all.merge(median_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(mean_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(standard_deviations, on='Sample', how='left')\n",
    "df_all = df_all.merge(variance_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(kurtosis_values, on='Sample', how='left')\n",
    "\n",
    "# Calculate rate of change for each group\n",
    "df_all['ROCEIMMagnitude'] = df_all['RollingAverageMag'].pct_change()\n",
    "df_all['ROCEIMPhase'] = df_all['RollingAveragePhase'].pct_change()\n",
    "\n",
    "#Filling NaN values out with the first ROC_value in the series\n",
    "ROC_value=df_all['ROCEIMMagnitude'][1]\n",
    "df_all['ROCEIMMagnitude'].fillna(value=ROC_value, inplace=True)\n",
    "ROC_value=df_all['ROCEIMPhase'][1]\n",
    "df_all['ROCEIMPhase'].fillna(value=ROC_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming your data is stored in a pandas DataFrame named 'df'\n",
    "# Filter the data for the first 10 samples\n",
    "df_last_10_samples = df_all[df_all['Sample'] >= 89]\n",
    "\n",
    "# Create a scatter plot for EIMMagnitude and JointAngle\n",
    "fig = go.Figure()\n",
    "\n",
    "for sample in df_last_10_samples['Sample'].unique():\n",
    "    sample_data = df_last_10_samples[df_last_10_samples['Sample'] == sample]\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['EIMMagnitude'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - EIMMagnitude'))\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['JointAngle'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - JointAngle'))\n",
    "\n",
    "fig.update_layout(title='EMG and Kinematic Data for the last 10 Samples',\n",
    "                  xaxis_title='Time Steps', yaxis_title='Value')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create test set<h3>\n",
    "These will be extracted from dataframe after normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_all.groupby(['Sample'])\n",
    "\n",
    "specific_samples = [23, 38, 47, 50, 89, 98]\n",
    "\n",
    "# Create an empty DataFrame to store the selected samples\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "# Iterate through the specific samples and extract them\n",
    "for sample_value in specific_samples:\n",
    "    if sample_value in df_grouped.groups:\n",
    "        df_test = pd.concat([df_test, df_grouped.get_group(sample_value)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_dir = 'test_samples.csv'\n",
    "# df_test.to_csv(df_test_dir, index= False)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns you want to normalize\n",
    "features_to_normalize = ['EIMMagnitude', 'EIMPhase', 'RollingAverageMag', 'RollingAveragePhase',\n",
    "                        'MedianEIMMagnitude', 'MedianEIMPhase', 'MeanEIMMagnitude',\n",
    "                        'MeanEIMPhase', 'StdEIMMagnitude', 'StdEIMPhase', 'VarEIMMagnitude',\n",
    "                        'VarEIMPhase', 'KurtEIMMagnitude', 'KurtEIMPhase', 'ROCEIMMagnitude',\n",
    "                        'ROCEIMPhase']\n",
    "\n",
    "targets_to_normalize = ['Mass', 'JointAngle']\n",
    "\n",
    "columns_to_use = ['EIMMagnitude', 'EIMPhase', 'RollingAverageMag', 'RollingAveragePhase',\n",
    "                        'MedianEIMMagnitude', 'MedianEIMPhase', 'MeanEIMMagnitude',\n",
    "                        'MeanEIMPhase', 'StdEIMMagnitude', 'StdEIMPhase', 'VarEIMMagnitude',\n",
    "                        'VarEIMPhase', 'KurtEIMMagnitude', 'KurtEIMPhase', 'ROCEIMMagnitude',\n",
    "                        'ROCEIMPhase', 'Mass', 'JointAngle']\n",
    "\n",
    "# Extract the 'Sample' column to append after normalization\n",
    "sample_column = df_all['Sample'].values\n",
    "\n",
    "# Create a copy of an original dataframe\n",
    "df2=df_all.drop(['Time', 'Sample'], axis=1)\n",
    "\n",
    "# Extracting mean and standard deviation for mean normalization\n",
    "df_mean = df2.mean()\n",
    "df_std = df2.std()\n",
    "normalized_df=(df2-df_mean)/df_std\n",
    "\n",
    "# Add the sample column again\n",
    "normalized_df['Sample'] = sample_column\n",
    "\n",
    "# Save means and std to CSV\n",
    "# df_mean.to_csv('/content/drive/MyDrive/NeuralNetwork/means.csv', header=True)\n",
    "# df_std.to_csv('/content/drive/MyDrive/NeuralNetwork/std_devs.csv', header=True)\n",
    "\n",
    "# Show a snapshot of data\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org = normalized_df*df_std+df_mean\n",
    "df_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original DataFrame length\n",
    "print(\"Original DataFrame Length:\", len(normalized_df))\n",
    "\n",
    "# Remove the extracted samples from the original DataFrame\n",
    "normalized_df.drop(normalized_df[normalized_df['Sample'] == 23].index, inplace = True)\n",
    "normalized_df.drop(normalized_df[normalized_df['Sample'] == 38].index, inplace = True)\n",
    "normalized_df.drop(normalized_df[normalized_df['Sample'] == 47].index, inplace = True)\n",
    "normalized_df.drop(normalized_df[normalized_df['Sample'] == 50].index, inplace = True)\n",
    "normalized_df.drop(normalized_df[normalized_df['Sample'] == 89].index, inplace = True)\n",
    "normalized_df.drop(normalized_df[normalized_df['Sample'] == 98].index, inplace = True)\n",
    "\n",
    "# Display the modified original DataFrame length\n",
    "print(\"\\nModified Original DataFrame Length:\", len(normalized_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if test samples have been removed\n",
    "specific_samples = [23, 38, 47, 50, 89, 98]\n",
    "\n",
    "for sample_value in specific_samples:\n",
    "  print(sample_value in normalized_df['Sample'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of features and sequence lengths\n",
    "num_features = 16\n",
    "sequence_length = 10\n",
    "\n",
    "columns_x = ['EIMMagnitude', 'EIMPhase', 'RollingAverageMag', 'RollingAveragePhase',\n",
    "            'MedianEIMMagnitude', 'MedianEIMPhase', 'MeanEIMMagnitude',\n",
    "            'MeanEIMPhase', 'StdEIMMagnitude', 'StdEIMPhase', 'VarEIMMagnitude',\n",
    "            'VarEIMPhase', 'KurtEIMMagnitude', 'KurtEIMPhase', 'ROCEIMMagnitude',\n",
    "            'ROCEIMPhase']\n",
    "\n",
    "columns_y = ['JointAngle', 'Mass']\n",
    "\n",
    "# Group the data by the 'Sample' column\n",
    "df_grouped = normalized_df.groupby(['Sample'])\n",
    "\n",
    "# Create sequences for each group\n",
    "X_seq, y_seq = [], []\n",
    "\n",
    "for group_name, group_data in df_grouped:\n",
    "\n",
    "    group_data_x_temp = group_data[columns_x]\n",
    "    # Convert to NumPy array\n",
    "    group_data_x = np.array(group_data_x_temp)\n",
    "\n",
    "    group_data_temp_y = group_data[columns_y]\n",
    "    # Convert to NumPy array\n",
    "    group_data_y = np.array(group_data_temp_y)\n",
    "\n",
    "    # Create sequences\n",
    "    for i in range(len(group_data) - sequence_length + 1):\n",
    "        X_seq.append(group_data_x[i:i+sequence_length, :])\n",
    "        y_seq.append(group_data_y[i+sequence_length-1, :])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_seq_np = np.array(X_seq)\n",
    "y_seq_np = np.array(y_seq)\n",
    "\n",
    "# Split into training and validation data in a 80/20 ratio. Testing is done with the samples extracted from the dataset earlier\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_seq_np, y_seq_np, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# Assuming loaded_model_path is the path to your saved model file\n",
    "loaded_model_path = 'C:/Users/Simons Lenovo/Documents/GitHub/MUSE/Code/Implementation/second_L2_Batch_16feat_30ts_50ep.keras'\n",
    "\n",
    "# Load the model with custom_objects to recognize the GRU layer\n",
    "loaded_model = load_model(loaded_model_path)\n",
    "\n",
    "print(\"Model loaded successfully.\")\n",
    "print(loaded_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyper parameter tuning</h1> \n",
    "\n",
    "# Introduction to Hyperparameter Sweeps\n",
    "\n",
    "Searching through high dimensional hyperparameter spaces to find the most performant model can get unwieldy very fast. Hyperparameter sweeps provide an organized and efficient way to conduct a battle royale of models and pick the most accurate model. They enable this by automatically searching through combinations of hyperparameter values (e.g. learning rate, batch size, number of hidden layers, optimizer type) to find the most optimal values.\n",
    "\n",
    "In this tutorial we'll see how you can run sophisticated hyperparameter sweeps in 3 easy steps using Weights and Biases.\n",
    "\n",
    "![](https://i.imgur.com/WVKkMWw.png)\n",
    "\n",
    "## Sweeps: An Overview\n",
    "\n",
    "Running a hyperparameter sweep with Weights & Biases is very easy. There are just 3 simple steps:\n",
    "\n",
    "1. **Define the sweep:** we do this by creating a dictionary or a [YAML file](https://docs.wandb.com/library/sweeps/configuration) that specifies the parameters to search through, the search strategy, the optimization metric et all.\n",
    "\n",
    "2. **Initialize the sweep:** with one line of code we initialize the sweep and pass in the dictionary of sweep configurations:\n",
    "`sweep_id = wandb.sweep(sweep_config)`\n",
    "\n",
    "3. **Run the sweep agent:** also accomplished with one line of code, we call wandb.agent() and pass the sweep_id to run, along with a function that defines your model architecture and trains it:\n",
    "`wandb.agent(sweep_id, function=train)`\n",
    "\n",
    "And voila! That's all there is to running a hyperparameter sweep! In the notebook below, we'll walk through these 3 steps in more detail.\n",
    "\n",
    "\n",
    "We highly encourage you to fork this notebook, tweak the parameters, or try the model with your own dataset!\n",
    "\n",
    "## Resources\n",
    "- [Sweeps docs →](https://docs.wandb.com/library/sweeps)\n",
    "- [Launching from the command line →](https://www.wandb.com/articles/hyperparameter-tuning-as-easy-as-1-2-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Start out by installing the experiment tracking library and setting up your free W&B account:\n",
    "\n",
    "\n",
    "*   **pip install wandb** – Install the W&B library\n",
    "*   **import wandb** – Import the wandb library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB – Install the W&B library\n",
    "# %pip install wandb -q\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install wandb -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, GRU, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop, SGD, Adam, Nadam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.initializers import he_normal\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For first tries: Further split the data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Sweep\n",
    "\n",
    "Weights & Biases sweeps give you powerful levers to configure your sweeps exactly how you want them, with just a few lines of code. The sweeps config can be defined as a dictionary or a [YAML file](https://docs.wandb.com/library/sweeps).\n",
    "\n",
    "Let's walk through some of them together:\n",
    "*   **Metric** – This is the metric the sweeps are attempting to optimize. Metrics can take a `name` (this metric should be logged by your training script) and a `goal` (maximize or minimize).\n",
    "*   **Search Strategy** – Specified using the 'method' variable. We support several different search strategies with sweeps.\n",
    "  *   **Grid Search** – Iterates over every combination of hyperparameter values.\n",
    "  *   **Random Search** – Iterates over randomly chosen combinations of hyperparameter values.\n",
    "  *   **Bayesian Search** – Creates a probabilistic model that maps hyperparameters to probability of a metric score, and chooses parameters with high probability of improving the metric. The objective of Bayesian optimization is to spend more time in picking the hyperparameter values, but in doing so trying out fewer hyperparameter values.\n",
    "*   **Stopping Criteria** – The strategy for determining when to kill off poorly peforming runs, and try more combinations faster. We offer several custom scheduling algorithms like [HyperBand](https://arxiv.org/pdf/1603.06560.pdf) and Envelope.\n",
    "*   **Parameters** – A dictionary containing the hyperparameter names, and discreet values, max and min values or distributions from which to pull their values to sweep over.\n",
    "\n",
    "You can find a list of all configuration options [here](https://docs.wandb.com/library/sweeps/configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the search mode. Choices: grid, random, bayes.\n",
    "# Use random over grid if you have many hyperparemeters to tune, since grid is computational heavy\n",
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To tell the sweep which metric to optimize and in which direction. Only necesary for bayes, but a good idea regardless\n",
    "metric = {\n",
    "    'name': 'accuracy',\n",
    "    'goal': 'maximize'\n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying hyperparameters we need to optimize\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'nadam', 'sgd', 'rmsprop']\n",
    "        },\n",
    "    'learning_rate': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0,\n",
    "        'max': 0.1\n",
    "        },\n",
    "    'batch_size': {\n",
    "        # integers between 32 and 256\n",
    "        # with evenly-distributed logarithms\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q': 8,\n",
    "        'min': 32,\n",
    "        'max': 256,\n",
    "        },\n",
    "    # 'fc_layer_size1': {\n",
    "    #     'values': [32, 64, 128, 256]\n",
    "    #     },\n",
    "    # 'fc_layer_size2': {\n",
    "    #     'values': [32, 64, 128, 256]\n",
    "    #     },\n",
    "    # 'dropout': {\n",
    "    #     'values': [0.2, 0.3, 0.4, 0.5]\n",
    "    #     },\n",
    "    # 'reccurent_dropout': {\n",
    "    #     'values': [0.2, 0.3, 0.4, 0.5]\n",
    "    #     },\n",
    "    # 'kernel_regularizer': {\n",
    "    #     # a flat distribution between 0 and 0.1\n",
    "    #     'distribution': 'uniform',\n",
    "    #     'min': 0.01,\n",
    "    #     'max': 0.4\n",
    "    # }\n",
    "\n",
    "      \n",
    "    # 'fc_layer_size': {\n",
    "    #     'values': [128, 256, 512]\n",
    "    #     },\n",
    "    # 'dropout': {\n",
    "    #       'values': [0.3, 0.4, 0.5]\n",
    "    #     },\n",
    "    # 'activation': {\n",
    "    #     'values': ['relu', 'elu', 'selu', 'softmax']\n",
    "    #     },\n",
    "    # 'weight_decay': {\n",
    "    #     'values': [0.0005, 0.005, 0.05]\n",
    "    #     }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the sweep – specify the parameters to search through, the search strategy, the optimization metric et all.\n",
    "# sweep_config = {\n",
    "#     'method': 'random', #grid, random\n",
    "#     'metric': {\n",
    "#       'name': 'accuracy',\n",
    "#       'goal': 'maximize'\n",
    "#     },\n",
    "#     'parameters': {\n",
    "#         'epochs': {\n",
    "#             'values': [2, 5, 10]\n",
    "#         },\n",
    "#         'batch_size': {\n",
    "#             'values': [256, 128, 64, 32]\n",
    "#         },\n",
    "#         'dropout': {\n",
    "#             'values': [0.3, 0.4, 0.5]\n",
    "#         },\n",
    "#         'fc_layer_size': {\n",
    "#             'values': [16, 32, 64]\n",
    "#         },\n",
    "#         'weight_decay': {\n",
    "#             'values': [0.0005, 0.005, 0.05]\n",
    "#         },\n",
    "#         'learning_rate': {\n",
    "#             'values': [1e-2, 1e-3, 1e-4, 3e-4, 3e-5, 1e-5]\n",
    "#         },\n",
    "#         'optimizer': {\n",
    "#             'values': ['adam', 'nadam', 'sgd', 'rmsprop']\n",
    "#         },\n",
    "#         'activation': {\n",
    "#             'values': ['relu', 'elu', 'selu', 'softmax']\n",
    "#         }\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For setting values that we don't want to vary in the script.\n",
    "parameters_dict.update({\n",
    "    'epochs': {\n",
    "        'value': 10}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new sweep\n",
    "# Arguments:\n",
    "#     – sweep_config: the sweep config dictionary defined above\n",
    "#     – entity: Set the username for the sweep\n",
    "#     – project: Set the project name for the sweep\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'C:/Users/Simons Lenovo/Documents/GitHub/MUSE/Code/GRU_network/network.ipynb'\n",
    "sweep_id = wandb.sweep(sweep_config, entity=\"simoncv\", project=\"sweeps-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Your Neural Network\n",
    "Before we can run the sweep, let's define a function that creates and trains our neural network.\n",
    "\n",
    "In the function below, we define a simplified version of a VGG19 model in Keras, and add the following lines of code to log models metrics, visualize performance and output and track our experiments easily:\n",
    "*   **wandb.init()** – Initialize a new W&B run. Each run is single execution of the training script.\n",
    "*   **wandb.config** – Save all your hyperparameters in a config object. This lets you use our app to sort and compare your runs by hyperparameter values.\n",
    "*   **callbacks=[WandbCallback()]** – Fetch all layer dimensions, model parameters and log them automatically to your W&B dashboard.\n",
    "*   **wandb.log()** – Logs custom objects – these can be images, videos, audio files, HTML, plots, point clouds etc. Here we use wandb.log to log images of Simpson characters overlaid with actual and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sweep calls this function with each set of hyperparameters\n",
    "def train():\n",
    "    # Default values for hyper-parameters we're going to sweep over\n",
    "    config_defaults = {\n",
    "        'epochs': 10,\n",
    "        'batch_size': 128,\n",
    "        'weight_decay': 0.0005,\n",
    "        'learning_rate': 1e-3,\n",
    "        'activation': 'relu',\n",
    "        'optimizer': 'adam',\n",
    "        'fc_layer_size1': 32,\n",
    "        'fc_layer_size2': 64,\n",
    "        'dropout': 0.2,\n",
    "        'reccurent_dropout': 0.4,\n",
    "        'kernel_regularizer': 0.01,\n",
    "        'momentum': 0.9,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    # Initialize a new wandb run\n",
    "    wandb.init(config=config_defaults)\n",
    "\n",
    "    # Config is a variable that holds and saves hyperparameters and inputs\n",
    "    config = wandb.config\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    seed_value = 42\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "\n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(GRU(units=config.fc_layer_size1, activation='relu', return_sequences=True,\n",
    "                  kernel_initializer=he_normal(seed=seed_value),\n",
    "                  kernel_regularizer=l2(config.kernel_regularizer),\n",
    "                  input_shape=(sequence_length, num_features), name='Input-Layer'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(config.dropout, seed=seed_value))  # Dropout after the first GRU layer\n",
    "\n",
    "    model.add(GRU(units = config.fc_layer_size2, activation='relu',\n",
    "                  kernel_initializer=he_normal(seed=seed_value),\n",
    "                  kernel_regularizer=l2(config.kernel_regularizer),\n",
    "                  recurrent_dropout=config.reccurent_dropout, dropout=config.dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(config.dropout, seed=seed_value))  # Dropout after the second GRU layer\n",
    "\n",
    "    model.add(Dense(units=2, activation='linear'))  # Output layer\n",
    "\n",
    "    # Define the optimizer\n",
    "    if config.optimizer=='sgd':\n",
    "      optimizer = SGD(learning_rate=config.learning_rate, weight_decay=1e-5, momentum=config.momentum, nesterov=True)\n",
    "    elif config.optimizer=='rmsprop':\n",
    "      optimizer = RMSprop(learning_rate=config.learning_rate, weight_decay=1e-5)\n",
    "    elif config.optimizer=='adam':\n",
    "      optimizer = Adam(learning_rate=config.learning_rate, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "    elif config.optimizer=='nadam':\n",
    "      optimizer = Nadam(learning_rate=config.learning_rate, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "\n",
    "    model.compile(loss = \"mean_squared_error\", optimizer = optimizer, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=config.batch_size,\n",
    "              epochs=config.epochs,\n",
    "              validation_data=(X_val, y_val),\n",
    "              callbacks=[WandbCallback(data_type=\"graph\", validation_data=(X_val, y_val)),\n",
    "                          EarlyStopping(patience=10, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the sweep agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new sweep\n",
    "# Arguments:\n",
    "#     – sweep_id: the sweep_id to run - this was returned above by wandb.sweep()\n",
    "#     – function: function that defines your model architecture and trains it\n",
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature engineering</h1> \n",
    "Calculating median, mean, standard deviation, variance and kurtosis for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas options to display more columns\n",
    "pd.options.display.max_columns=150\n",
    "\n",
    "# Calculate the median for each group\n",
    "median_values = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].apply(lambda group: group[['EIMMagnitude', 'EIMPhase', 'JointAngle']].agg(['min', 'max']).median())\n",
    "\n",
    "# Calculate the mean for each group\n",
    "mean_values = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].mean()\n",
    "\n",
    "# Calculate the standard deviation for each group\n",
    "standard_deviations = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].std()\n",
    "\n",
    "# Calculate the variance for each group\n",
    "variance_values = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].var()\n",
    "\n",
    "# Calculate the kurtosis for each group\n",
    "kurtosis_values = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].apply(pd.DataFrame.kurtosis)\n",
    "\n",
    "# Reset the index to get the 'Sample' column back\n",
    "median_values.reset_index(inplace=True)\n",
    "mean_values.reset_index(inplace=True)\n",
    "standard_deviations.reset_index(inplace=True)\n",
    "variance_values.reset_index(inplace=True)\n",
    "kurtosis_values.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns to indicate they represent the median\n",
    "median_values.columns = ['Sample', 'MedianEIMMagnitude', 'MedianEIMPhase', 'MedianJointAngle']\n",
    "mean_values.columns = ['Sample', 'MeanEIMMagnitude', 'MeanEIMPhase', 'MeanJointAngle']\n",
    "standard_deviations.columns = ['Sample', 'StdEIMMagnitude', 'StdEIMPhase', 'StdJointAngle']\n",
    "variance_values.columns = ['Sample', 'VarEIMMagnitude', 'VarEIMPhase', 'VarJointAngle']\n",
    "kurtosis_values.columns = ['Sample', 'KurtEIMMagnitude', 'KurtEIMPhase', 'KurtJointAngle']\n",
    "\n",
    "# Merge the median and mean values back into the original DataFrame based on the 'Sample' column\n",
    "df_all = df_all.merge(median_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(mean_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(standard_deviations, on='Sample', how='left')\n",
    "df_all = df_all.merge(variance_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(kurtosis_values, on='Sample', how='left')\n",
    "\n",
    "# Calculate rate of change for each group\n",
    "df_all['ROCEIMMagnitude'] = df_all['RollingAverageMag'].pct_change()\n",
    "df_all['ROCEIMPhase'] = df_all['RollingAveragePhase'].pct_change()\n",
    "df_all['ROCJointAngle'] = df_all['JointAngle'].pct_change()\n",
    "\n",
    "#Filling NaN values out with the first mean value in the series\n",
    "ROC_value=df_all['ROCEIMMagnitude'][1]\n",
    "df_all['ROCEIMMagnitude'].fillna(value=ROC_value, inplace=True)\n",
    "ROC_value=df_all['ROCEIMPhase'][1]\n",
    "df_all['ROCEIMPhase'].fillna(value=ROC_value, inplace=True)\n",
    "ROC_value=df_all['ROCJointAngle'][1]\n",
    "df_all['ROCJointAngle'].fillna(value=ROC_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Visualization</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming your data is stored in a pandas DataFrame named 'df'\n",
    "# Filter the data for the first 10 samples\n",
    "df_last_10_samples = df_all[df_all['Sample'] >= 89]\n",
    "\n",
    "# Create a scatter plot for EIMMagnitude and JointAngle\n",
    "fig = go.Figure()\n",
    "\n",
    "for sample in df_last_10_samples['Sample'].unique():\n",
    "    sample_data = df_last_10_samples[df_last_10_samples['Sample'] == sample]\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['EIMMagnitude'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - EIMMagnitude'))\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['JointAngle'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - JointAngle'))\n",
    "\n",
    "fig.update_layout(title='EMG and Kinematic Data for the last 10 Samples',\n",
    "                  xaxis_title='Time Steps', yaxis_title='Value')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data preprocessing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing. Range of the scaler should be the same for x and y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of an original dataframe\n",
    "df2=df_all.copy()\n",
    "\n",
    "# Define the columns you want to normalize\n",
    "columns_to_normalize = ['EIMMagnitude', 'EIMPhase',\t'JointAngle', 'RollingAverageMag', 'RollingAveragePhase', \n",
    "                        'MedianEIMMagnitude', 'MedianEIMPhase', 'MedianJointAngle', 'MeanEIMMagnitude',\n",
    "                        'MeanEIMPhase', 'MeanJointAngle', 'StdEIMMagnitude', 'StdEIMPhase',\t'StdJointAngle',\n",
    "                        'VarEIMMagnitude', 'VarEIMPhase', 'VarJointAngle', 'KurtEIMMagnitude', 'KurtEIMPhase',\n",
    "                        'KurtJointAngle', 'ROCEIMMagnitude', 'ROCEIMPhase', 'ROCJointAngle']\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "df2[columns_to_normalize] = scaler.fit_transform(df_all[columns_to_normalize])\n",
    "\n",
    "# Show a snaphsot of data\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df2.groupby(['Sample'])\n",
    "\n",
    "# Display the original DataFrame length\n",
    "print(\"Original DataFrame Length:\", len(df2))\n",
    "\n",
    "specific_samples = [23, 38, 47, 50, 89, 98]\n",
    "\n",
    "# Create an empty DataFrame to store the selected samples\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "# Iterate through the specific samples and extract them\n",
    "for sample_value in specific_samples:\n",
    "    if sample_value in df_grouped.groups:\n",
    "        df_test = pd.concat([df_test, df_grouped.get_group(sample_value)])\n",
    "\n",
    "# Remove the extracted samples from the original DataFrame\n",
    "df2.drop(df2[df2['Sample'] == 23].index, inplace = True)\n",
    "df2.drop(df2[df2['Sample'] == 38].index, inplace = True)\n",
    "df2.drop(df2[df2['Sample'] == 47].index, inplace = True)\n",
    "df2.drop(df2[df2['Sample'] == 50].index, inplace = True)\n",
    "df2.drop(df2[df2['Sample'] == 89].index, inplace = True)\n",
    "df2.drop(df2[df2['Sample'] == 98].index, inplace = True)\n",
    "\n",
    "# Display the modified original DataFrame length\n",
    "print(\"\\nModified Original DataFrame Length:\", len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_value in specific_samples:\n",
    "  print(sample_value in df2['Sample'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data is stored in a pandas DataFrame named 'df'\n",
    "# Filter the data for the first 10 samples\n",
    "df_last_10_samples = df2[df2['Sample'] >= 89]\n",
    "\n",
    "# Create a scatter plot for EIMMagnitude and JointAngle\n",
    "fig = go.Figure()\n",
    "\n",
    "for sample in df_last_10_samples['Sample'].unique():\n",
    "    sample_data = df_last_10_samples[df_last_10_samples['Sample'] == sample]\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['EIMMagnitude'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - EIMMagnitude'))\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['JointAngle'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - JointAngle'))\n",
    "\n",
    "fig.update_layout(title='EMG and Kinematic Data for the First 10 Samples',\n",
    "                  xaxis_title='Time Steps', yaxis_title='Value')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prepare sequences for GRU</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important for synchronizing the data with eachother with a common time variable. The \"time-variable\" from the dataset is therefor strictly speaking not needed for training the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training and evaluating GRU model.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_gru(datain, timestep, scaler, columns_to_use):\n",
    "    X_out, Y_out = None, None\n",
    "\n",
    "    for sample in datain['Sample'].unique():\n",
    "        datatmp = datain[datain['Sample'] == sample].copy()\n",
    "\n",
    "        for col in columns_to_use:\n",
    "            arr = datatmp[col].to_numpy()\n",
    "\n",
    "            # Scale using transform (using previously fitted scaler)\n",
    "            arr_scaled = scaler.transform(arr.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Use numpy's stride tricks to create the sequences\n",
    "            shape = (len(arr_scaled) - 2 * timestep + 1, timestep, 1)\n",
    "            strides = (arr_scaled.itemsize, arr_scaled.itemsize, arr_scaled.itemsize)\n",
    "            X_tmp = np.lib.stride_tricks.as_strided(arr_scaled, shape=shape, strides=strides)\n",
    "\n",
    "            # Reshape for GRU input\n",
    "            X_tmp = X_tmp.reshape(-1, timestep, 1)\n",
    "\n",
    "            # Create corresponding Y sequences\n",
    "            Y_tmp = arr_scaled[timestep:2 * timestep].reshape(-1, timestep, 1)\n",
    "\n",
    "            if X_out is None:\n",
    "                X_out = X_tmp\n",
    "                Y_out = Y_tmp\n",
    "            else:\n",
    "                X_out = np.concatenate((X_out, X_tmp), axis=0)\n",
    "                Y_out = np.concatenate((Y_out, Y_tmp), axis=0)\n",
    "\n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data and reshaping it for GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_path = 'EIM_kin_data.csv'\n",
    "# df_test=pd.read_csv(test_path, encoding='utf-8')\n",
    "\n",
    "thousand_datapoints = df_all.iloc[0:4000]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "timestep = 10\n",
    "\n",
    "# Split data into train, eval, and test dataframes\n",
    "train_ratio = 0.7\n",
    "eval_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Split the data\n",
    "df_train, df_temp = train_test_split(thousand_datapoints, test_size=1 - train_ratio, random_state=42)\n",
    "df_eval, df_test = train_test_split(df_temp, test_size=test_ratio / (test_ratio + eval_ratio), random_state=42)\n",
    "\n",
    "# 'EIMMagnitude', 'EIMPhase',\t'JointAngle', 'Mass', 'RollingAverageMag', 'RollingAveragePhase', \n",
    "#                         'MedianEIMMagnitude', 'MedianEIMPhase', 'MedianJointAngle', 'MeanEIMMagnitude',\n",
    "#                         'MeanEIMPhase', 'MeanJointAngle', 'StdEIMMagnitude', 'StdEIMPhase',\t'StdJointAngle',\n",
    "#                         'VarEIMMagnitude', 'VarEIMPhase', 'VarJointAngle', 'KurtEIMMagnitude', 'KurtEIMPhase',\n",
    "#                         'KurtJointAngle', 'ROCEIMMagnitude', 'ROCEIMPhase', 'ROCJointAngle'\n",
    "\n",
    "columns_to_train_on = ['EIMMagnitude', 'EIMPhase',\t'JointAngle', 'Mass', 'RollingAverageMag', 'RollingAveragePhase', \n",
    "                        'MedianEIMMagnitude', 'MedianEIMPhase', 'MedianJointAngle', 'MeanEIMMagnitude']\n",
    "\n",
    "# Use fit to train the scaler on the training data only, actual scaling will be done inside reshaping function\n",
    "scaler.fit(df_train[columns_to_train_on].values.reshape(-1, 1))\n",
    "\n",
    "# Use the reshaping function to reshape the data for GRU\n",
    "columns_to_use = columns_to_train_on\n",
    "\n",
    "X_train, Y_train = reshape_for_gru(df_train, timestep, scaler, columns_to_use)\n",
    "X_eval, Y_eval = reshape_for_gru(df_eval, timestep, scaler, columns_to_use)\n",
    "X_test, Y_test = reshape_for_gru(df_test, timestep, scaler, columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train_shape: ',X_train.shape)\n",
    "print('Y_train_shape: ',Y_train.shape)\n",
    "print('X_eval_shape: ',X_eval.shape)\n",
    "print('Y_eval_shape: ',Y_eval.shape)\n",
    "print('X_test_shape: ',X_test.shape)\n",
    "print('Y_test_shape: ',Y_test.shape)\n",
    "\n",
    "print(Y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Specify the structure of a Neural Network<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name=\"GRU-Model\")\n",
    "model.add(Input(shape=(X_train.shape[1],X_train.shape[2]), name='Input-Layer'))\n",
    "model.add(Bidirectional(GRU(units=32, activation='tanh', recurrent_activation='sigmoid', stateful=False), name='Hidden-GRU-Encoder-Layer'))\n",
    "model.add(RepeatVector(X_train.shape[1], name='Repeat-Vector-Layer'))\n",
    "model.add(Bidirectional(GRU(units=32, activation='tanh', recurrent_activation='sigmoid', stateful=False, return_sequences=True), name='Hidden-GRU-Decoder-Layer'))\n",
    "model.add(TimeDistributed(Dense(units=1, activation='linear'), name='Output-Layer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compile the model<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['MeanSquaredError', 'MeanAbsoluteError'],\n",
    "              loss_weights=None,\n",
    "              weighted_metrics=None,\n",
    "              run_eagerly=None,\n",
    "              steps_per_execution=None\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fit the model on the dataset<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    callbacks=None,\n",
    "                    validation_data=(X_eval, Y_eval),\n",
    "                    shuffle=True,\n",
    "                    class_weight=None,\n",
    "                    sample_weight=None,\n",
    "                    initial_epoch=0,\n",
    "                    steps_per_epoch=None,\n",
    "                    validation_steps=None,\n",
    "                    validation_batch_size=None,\n",
    "                    validation_freq=10,\n",
    "                    max_queue_size=10,\n",
    "                    workers=1,\n",
    "                    use_multiprocessing=True,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use model to make predictions<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict results on training data\n",
    "#pred_train = model.predict(X_train)\n",
    "# Predict results on test data\n",
    "pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Print Performance Summary<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print('-------------------- Model Summary --------------------')\n",
    "# print model summary\n",
    "model.summary()\n",
    "print(\"\")\n",
    "print('-------------------- Weights and Biases --------------------')\n",
    "print(\"Too many parameters to print but you can use the code provided if needed\")\n",
    "print(\"\")\n",
    "#for layer in model.layers:\n",
    "#    print(layer.name)\n",
    "#    for item in layer.get_weights():\n",
    "#        print(\"  \", item)\n",
    "#print(\"\")\n",
    "\n",
    "# Print the last value in the evaluation metrics contained within history file\n",
    "print('-------------------- Evaluation on Training Data --------------------')\n",
    "for item in history.history:\n",
    "    print(\"Final\", item, \":\", history.history[item][-1])\n",
    "print(\"\")\n",
    "\n",
    "# Evaluate the model on the test data using \"evaluate\"\n",
    "print('-------------------- Evaluation on Test Data --------------------')\n",
    "results = model.evaluate(X_test, Y_test)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "\n",
    "# Create a plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for actual values (ground truth)\n",
    "for i in range(X_test):\n",
    "    fig.add_trace(go.Scatter(x=np.arange(len(Y_test[i])), y=scaler.inverse_transform(Y_test[i].reshape(-1, 1)).flatten(),\n",
    "                             mode='lines',\n",
    "                             name=f'Sample {i + 1} - Actual',\n",
    "                             opacity=0.8,\n",
    "                             line=dict(width=1)\n",
    "                            ))\n",
    "\n",
    "# Add traces for predicted values\n",
    "for i in range(X_test):\n",
    "    fig.add_trace(go.Scatter(x=np.arange(len(pred_test[i])), y=scaler.inverse_transform(pred_test[i].reshape(-1, 1)).flatten(),\n",
    "                             mode='lines',\n",
    "                             name=f'Sample {i + 1} - Predicted',\n",
    "                             opacity=1,\n",
    "                             line=dict(width=2, dash='dot')\n",
    "                            ))\n",
    "\n",
    "# Customize the plot appearance\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white',\n",
    "    xaxis=dict(\n",
    "        showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "        zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "        showline=True, linewidth=1, linecolor='black',\n",
    "        title='Time Steps'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "        zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "        showline=True, linewidth=1, linecolor='black',\n",
    "        title='Values'\n",
    "    ),\n",
    "    title=dict(text=\"Actual vs. Predicted Values\", font=dict(color='black'))\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Splitting the data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the data into a training/evaluation/test distribution of 70/20/10. The random_state parameter is a seed for the random split, that allows reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kinematic_features = 18\n",
    "num_emg_features = 7\n",
    "\n",
    "# Extract X_kinematic and y_kinematic\n",
    "# Adjust num_kinematic_features\n",
    "X_kinematic = kinematic_data_normalized[:, :, :num_kinematic_features]\n",
    "# Assuming the last time step represents the target\n",
    "y_kinematic = kinematic_data_normalized[:, -1, :]\n",
    "\n",
    "# Extract X_emg and y_emg\n",
    "# Adjust num_emg_features\n",
    "X_emg = emg_data_normalized[:, :, :num_emg_features]\n",
    "# Assuming the last time step represents the target\n",
    "y_emg = emg_data_normalized[:, -1, :]\n",
    "\n",
    "# Now you have X_kinematic, y_kinematic, X_emg, and y_emg for further processing\n",
    "\n",
    "# Split the data\n",
    "X_kinematic_train, X_kinematic_val, y_kinematic_train, y_kinematic_val = train_test_split(\n",
    "    X_kinematic, y_kinematic, test_size=0.3, random_state=42\n",
    ")\n",
    "X_emg_train, X_emg_val, y_emg_train, y_emg_val = train_test_split(\n",
    "    X_emg, y_emg, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Ensure shapes match the model input requirements\n",
    "# Add additional processing steps if necessary\n",
    "\n",
    "# Print the shapes for verification\n",
    "# print(\"Shapes of Kinematic Data Sets:\")\n",
    "# print(\"Train:\", X_train_kinematic.shape, y_train_kinematic.shape)\n",
    "# print(\"Validation:\", X_val_kinematic.shape, y_val_kinematic.shape)\n",
    "# print(\"Test:\", X_test_kinematic.shape, y_test_kinematic.shape)\n",
    "\n",
    "# print(\"\\nShapes of EMG Data Sets:\")\n",
    "# print(\"Train:\", X_train_emg.shape, y_train_emg.shape)\n",
    "# print(\"Validation:\", X_val_emg.shape, y_val_emg.shape)\n",
    "# print(\"Test:\", X_test_emg.shape, y_test_emg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model architecture</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Masking, RepeatVector, Layer, Reshape\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Assuming num_emg_features, num_kinematic_features, and num_joint_angle_features are defined\n",
    "num_emg_features = 7\n",
    "num_kinematic_features = 18\n",
    "\n",
    "# Total number of features\n",
    "num_input_features = num_emg_features + num_kinematic_features\n",
    "\n",
    "# Define the input sequence shape\n",
    "# Variable-length input sequence\n",
    "input_seq_shape = (None, num_input_features)\n",
    "\n",
    "# Define the GRU units\n",
    "gru_units = 32\n",
    "\n",
    "# EMG data branch\n",
    "encoder_inputs_emg = Input(shape=(None, num_emg_features), name='Input-Layer-EMG')\n",
    "encoder_emg = GRU(gru_units, return_sequences=True, name='Hidden-GRU-Encoder-Layer-EMG')(encoder_inputs_emg)\n",
    "\n",
    "# Kinematic data branch\n",
    "encoder_inputs_kinematic = Input(shape=(None, num_kinematic_features), name='Input-Layer-Kinematic')\n",
    "encoder_kinematic = GRU(gru_units, return_sequences=True, name='Hidden-GRU-Encoder-Layer-Kinematic')(encoder_inputs_kinematic)\n",
    "\n",
    "# Concatenate EMG and kinematic encodings\n",
    "encoder_combined = concatenate([encoder_emg, encoder_kinematic], axis=-1)\n",
    "\n",
    "# Attention Mechanism\n",
    "attention = Dot(axes=[1, 1], name='Attention-Layer')([encoder_combined, encoder_combined])\n",
    "attention = Activation('softmax', name='Attention-Activation')(attention)\n",
    "\n",
    "# Apply attention weights to encoder outputs\n",
    "context = Dot(axes=[1, 2], name='Context-Layer')([attention, encoder_combined])\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None, num_kinematic_features), name='Decoder-Input-Layer')\n",
    "decoder_gru = GRU(gru_units, return_sequences=True, name='Hidden-GRU-Decoder-Layer')(decoder_inputs)\n",
    "\n",
    "# Flatten context along with masking\n",
    "context_flattened = Flatten()(Masking()(context))\n",
    "\n",
    "# Custom layer to repeat the context along the time axis\n",
    "class RepeatContextLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RepeatContextLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.expand_dims(inputs, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 1, input_shape[1])\n",
    "\n",
    "context_expanded = RepeatContextLayer(name='Repeat-Context')(context_flattened)\n",
    "\n",
    "# Concatenate expanded context and decoder GRU output\n",
    "decoder_combined = Concatenate(axis=-1, name='Concatenate-Layer')([context_expanded, decoder_gru])\n",
    "\n",
    "# Flatten the decoder_combined while maintaining the sequence length\n",
    "decoder_flattened = Flatten(name='Flatten-Layer')(decoder_combined)\n",
    "\n",
    "print('Decoder shape before reshaping: ', decoder_flattened.shape)\n",
    "# Reshape to have a single dimension in the output\n",
    "decoder_output = Reshape((-1, 1), name='Reshape-Layer')(decoder_flattened)\n",
    "print('Flatten-Layer shape: ', decoder_output.shape)\n",
    "\n",
    "# Output layer\n",
    "outputs = TimeDistributed(Dense(1, activation='linear'), name='Output-Layer')(decoder_output)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[encoder_inputs_emg, encoder_inputs_kinematic, decoder_inputs], outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Print model summary for review\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST OF CODE IN COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming loaded_model_path is the path to your saved model file\n",
    "loaded_model_path = 'short_test_5_epoch.keras'\n",
    "\n",
    "# Load the model with custom_objects to recognize the GRU layer\n",
    "loaded_model = load_model(loaded_model_path)\n",
    "\n",
    "print(\"Model loaded successfully.\")\n",
    "print(loaded_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df_last_10_samples = df_all[df_all['Sample'] >= 89]\n",
    "test_eim = np.array(df_last_10_samples['EIMMagnitude'])\n",
    "print(test_eim)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "columns_to_normalize = ['EIMMagnitude', 'EIMPhase',\t'JointAngle', 'RollingAverageMag', 'RollingAveragePhase',\n",
    "                        'MedianEIMMagnitude', 'MedianEIMPhase', 'MedianJointAngle', 'MeanEIMMagnitude',\n",
    "                        'MeanEIMPhase', 'MeanJointAngle', 'StdEIMMagnitude', 'StdEIMPhase',\t'StdJointAngle',\n",
    "                        'VarEIMMagnitude', 'VarEIMPhase', 'VarJointAngle', 'KurtEIMMagnitude', 'KurtEIMPhase',\n",
    "                        'KurtJointAngle', 'ROCEIMMagnitude', 'ROCEIMPhase', 'ROCJointAngle']\n",
    "\n",
    "scaler.fit(df_last_10_samples[columns_to_normalize].values.reshape(-1,1))\n",
    "df_grouped = df_last_10_samples.groupby(['Sample'])\n",
    "\n",
    "# Split the dataset into input features (X) and target variables (y)\n",
    "num_features = 23\n",
    "# X = df_grouped[['EIMMagnitude', 'EIMPhase', 'RollingAverageMag', 'RollingAveragePhase','MedianEIMMagnitude',\n",
    "#         'MedianEIMPhase', 'MeanEIMMagnitude', 'MeanEIMPhase', 'StdEIMMagnitude', 'StdEIMPhase', 'VarEIMMagnitude',\n",
    "#         'VarEIMPhase', 'KurtEIMMagnitude', 'KurtEIMPhase', 'ROCEIMMagnitude', 'ROCEIMPhase']]\n",
    "# y = df_grouped[['JointAngle', 'Mass']]\n",
    "\n",
    "# Initialize scalers\n",
    "scalers_X = {}\n",
    "scalers_y = {}\n",
    "\n",
    "# Scale data and create sequences for each group\n",
    "X_seq, y_seq = [], []\n",
    "\n",
    "for group_name, group_data in df_grouped:\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    # Scale features\n",
    "    group_data_scaled = scaler.fit_transform(group_data[['JointAngle', 'MedianJointAngle', 'MeanJointAngle','StdJointAngle',\n",
    "                                                           'ROCJointAngle', 'VarJointAngle', 'KurtJointAngle',\n",
    "                                                           'EIMMagnitude', 'EIMPhase', 'RollingAverageMag', 'RollingAveragePhase',\n",
    "                                                           'MedianEIMMagnitude', 'MedianEIMPhase', 'MeanEIMMagnitude',\n",
    "                                                           'MeanEIMPhase', 'StdEIMMagnitude', 'StdEIMPhase', 'VarEIMMagnitude',\n",
    "                                                           'VarEIMPhase', 'KurtEIMMagnitude', 'KurtEIMPhase', 'ROCEIMMagnitude','ROCEIMPhase']])\n",
    "\n",
    "    # Scale target variables\n",
    "    group_data_scaled_y = scaler.fit_transform(group_data[['JointAngle', 'Mass']])\n",
    "\n",
    "    # Create sequences (adjust sequence_length as needed)\n",
    "    sequence_length = 10\n",
    "    for i in range(len(group_data) - sequence_length + 1):\n",
    "        X_seq.append(group_data_scaled[i:i+sequence_length, :])\n",
    "        y_seq.append(group_data_scaled_y[i+sequence_length-1, :])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "timestep = 10\n",
    "\n",
    "# X_seq, y_seqn = reshape_for_gru(df_last_10_samples, timestep, scaler, columns_to_use)\n",
    "\n",
    "predictions_scaled = loaded_model.predict(X_seq)\n",
    "predictions = scaler.inverse_transform(predictions_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
