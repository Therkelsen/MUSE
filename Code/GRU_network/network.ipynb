{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import GRU, Dense\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Assuming that you have your joint angles and EIM data in X and the corresponding labels (joint angle and weight) in y.\n",
    "\n",
    "# # Example data loading (replace this with your actual data loading code)\n",
    "# # X and y should be NumPy arrays\n",
    "# # X should have shape (num_samples, num_timesteps, num_features)\n",
    "# # y should have shape (num_samples, num_output_features)\n",
    "\n",
    "# # Generate example data (replace this with your actual data loading code)\n",
    "# np.random.seed(42)\n",
    "# num_samples = 1000\n",
    "# num_timesteps = 10\n",
    "# num_features = 2  # Replace with the actual number of features for joint angles and EIM data\n",
    "# num_output_features = 2  # Replace with the actual number of output features (joint angle and weight)\n",
    "\n",
    "# X = np.random.rand(num_samples, num_timesteps, num_features)\n",
    "# y = np.random.rand(num_samples, num_output_features)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Normalize the data using Min-Max scaling\n",
    "# scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "# X_train = scaler_x.fit_transform(X_train.reshape(-1, num_features)).reshape(X_train.shape)\n",
    "# X_test = scaler_x.transform(X_test.reshape(-1, num_features)).reshape(X_test.shape)\n",
    "\n",
    "# scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "# y_train = scaler_y.fit_transform(y_train)\n",
    "# y_test = scaler_y.transform(y_test)\n",
    "\n",
    "# # Build the GRU model\n",
    "# model = Sequential()\n",
    "# model.add(GRU(50, activation='relu', input_shape=(num_timesteps, num_features)))\n",
    "# model.add(Dense(num_output_features, activation='linear'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss = model.evaluate(X_test, y_test)\n",
    "# print(f'Test Loss: {loss}')\n",
    "\n",
    "# # Make predictions on new data\n",
    "# # Replace `new_data` with your actual new data\n",
    "# new_data = np.random.rand(1, num_timesteps, num_features)\n",
    "# scaled_new_data = scaler_x.transform(new_data.reshape(-1, num_features)).reshape(new_data.shape)\n",
    "# prediction = model.predict(scaled_new_data)\n",
    "# scaled_prediction = scaler_y.inverse_transform(prediction)\n",
    "# print(f'Predicted values: {scaled_prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Libaries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow\n",
    "# %pip install pandas\n",
    "# %pip install scikit-learn\n",
    "# %pip install plotly\n",
    "# %pip install scipy\n",
    "\n",
    "# Tensorflow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "from keras.models import Sequential # for creating a linear stack of layers for our Neural Network\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Bidirectional, GRU, RepeatVector, Dense, TimeDistributed, concatenate, Dot, Activation, Concatenate, Flatten # for creating layers inside the Neural Network\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd # for data manipulation\n",
    "print('pandas: %s' % pd.__version__) # print version\n",
    "import numpy as np # for data manipulation\n",
    "print('numpy: %s' % np.__version__) # print version\n",
    "\n",
    "# Sklearn\n",
    "import sklearn\n",
    "print('sklearn: %s' % sklearn.__version__) # print version\n",
    "from sklearn.preprocessing import MinMaxScaler # for feature scaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "print('plotly: %s' % plotly.__version__) # print version\n",
    "\n",
    "import scipy.io\n",
    "from scipy.interpolate import interp1d\n",
    "print('scipy: %s' % scipy.__version__) # print version\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>From several csv files</h3>\n",
    "This section synchronizes and concatinates the EIM and kinematic data through unix time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_mean(input_signal):\n",
    "    output_signal = input_signal.copy()\n",
    "    buffer = len(input_signal) // 50\n",
    "    running_sum = 0.0\n",
    "\n",
    "    for i in range(len(input_signal)):\n",
    "        running_sum += input_signal[i]\n",
    "\n",
    "        if i < buffer:\n",
    "            output_signal[i] = running_sum / float(i + 1)\n",
    "        else:\n",
    "            running_sum -= input_signal[i - buffer]\n",
    "            output_signal[i] = running_sum / float(buffer)\n",
    "\n",
    "    return output_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas options to display more columns\n",
    "pd.options.display.max_columns=150\n",
    "\n",
    "# Loading data and adding headers 'ElbowAngles' and 'Time'\n",
    "kinematic_data_dir = 'MoCap/dynamic/99_HRML_4kg_pd/elbow_angles.csv'\n",
    "eim_data_dir = 'MoCap/dynamic/99_HRML_4kg_pd/processed_output_data.csv'\n",
    "df_kin=pd.read_csv(kinematic_data_dir, encoding='utf-8', names=[\"ElbowAngles\", \"Time\"])\n",
    "df_eim=pd.read_csv(eim_data_dir, encoding='utf-8')\n",
    "\n",
    "# Stripping data of unwanted delimiters and converting to float\n",
    "df_kin['ElbowAngles'] = df_kin['ElbowAngles'].str.strip('[]').astype(float)\n",
    "df_kin['Time'] = df_kin['Time'].str.strip('[]').astype(str)\n",
    "df_kin['Time'] = df_kin['Time'].str.strip('\\'').astype(float)\n",
    "\n",
    "# Extracting Unix time\n",
    "kin_unix = df_kin['Time'].values\n",
    "eim_unix = df_eim['Time'].values\n",
    "\n",
    "# Extracting max and min Unix values of kin and using these to figure out where to slice the EIM data\n",
    "kin_min = kin_unix.min()\n",
    "kin_max = kin_unix.max()\n",
    "\n",
    "# Filter EIM data based on kinematic Unix time range\n",
    "df_eim = df_eim[(df_eim['Time'] >= kin_min) & (df_eim['Time'] <= kin_max)]\n",
    "\n",
    "# Creating timestamps. EIM samples at 1000hZ, so 1 timestamp will correspond to 1ms\n",
    "df_eim['Timestamp'] = np.linspace(0, (len(df_eim) - 1), len(df_eim))\n",
    "df_kin['Timestamp'] = np.linspace(0, (len(df_kin) - 1), len(df_kin))\n",
    "\n",
    "# Extract timestamps and kinematic data\n",
    "kinematic_timestamps = df_kin['Timestamp'].values\n",
    "kinematic_data = df_kin['ElbowAngles'].values\n",
    "eim_timestamps = df_eim['Timestamp'].values\n",
    "\n",
    "# Interpolation of the kinematic data to match the EIM data\n",
    "shape = eim_timestamps.shape\n",
    "kinematic_interpolated = np.empty(shape)\n",
    "kin_idx = 0\n",
    "step = math.floor(len(eim_timestamps)/len(kinematic_timestamps))\n",
    "step_remainder = len(eim_timestamps)/len(kinematic_timestamps) - step\n",
    "step_temp = 0\n",
    "temp = 0\n",
    "\n",
    "# Linear interpolation done by averaging between two points. Each data step is done based \n",
    "# on the integer difference between the lenghts of the datasets. For increased accuracy, \n",
    "# whenever the remainder of the division becomes equal to or greater than 1, 1 is added \n",
    "# to the step, and withdrawn from the counter.\n",
    "for i in range(0, len(kinematic_data), 1):\n",
    "    kinematic_interpolated[kin_idx] = kinematic_data[i]\n",
    "    if i < len(kinematic_data)-1:\n",
    "        temp = kinematic_data[i+1]\n",
    "        for j in range(kin_idx + 1, kin_idx+step, 1):\n",
    "            kinematic_interpolated[j] = (kinematic_interpolated[j-1] + temp) / 2\n",
    "    else:\n",
    "        temp = kinematic_data[i]\n",
    "        for j in range(kin_idx + 1, len(eim_timestamps), 1):\n",
    "            kinematic_interpolated[j] = (kinematic_interpolated[j-1] + temp) / 2\n",
    "\n",
    "        step_temp = step_temp+step_remainder\n",
    "    if step_temp >= 1:\n",
    "        kin_idx = kin_idx + step + 1\n",
    "        step_temp = step_temp - 1\n",
    "    else:\n",
    "        kin_idx = kin_idx + step\n",
    "\n",
    "\n",
    "# Interpolate kinematic data to match EIM timestamps. Lines are drawn between the spread out data. \n",
    "# Missing values are being extrapolated\n",
    "kinematic_interpolated = interp1d(eim_timestamps, kinematic_interpolated, kind='linear', fill_value='extrapolate')\n",
    "\n",
    "# The interpolated data is being saved to the JointAngle column in the eim_data\n",
    "df_eim['JointAngle'] = kinematic_interpolated(eim_timestamps)\n",
    "\n",
    "# df_eim['RollingAverageMag'] = df_eim['EIMMagnitude'].rolling(100).mean()\n",
    "# df_eim['RollingAveragePhase'] = df_eim['EIMPhase'].rolling(100).mean()\n",
    "\n",
    "# Calculating rolling mean\n",
    "\n",
    "df_eim['RollingAverageMag'] = rolling_mean(df_eim['EIMMagnitude'])\n",
    "df_eim['RollingAveragePhase'] = rolling_mean(df_eim['EIMPhase'])\n",
    "df_eim['RollingStdEIM'] = df_eim['EIMMagnitude'].rolling(100).std()\n",
    "std_value=df_eim['RollingStdEIM'][99]\n",
    "df_eim['RollingStdEIM'].fillna(value=std_value, inplace=True) \n",
    "\n",
    "#Filling NaN values out with the first mean value in the series\n",
    "# mean_value=df_eim['RollingAverageMag'][99]\n",
    "# df_eim['RollingAverageMag'].fillna(value=mean_value, inplace=True) \n",
    "# mean_value=df_eim['RollingAveragePhase'][99]\n",
    "# df_eim['RollingAveragePhase'].fillna(value=mean_value, inplace=True) \n",
    "\n",
    "\n",
    "# NB! THIS SECTION OF SAVING THE FILES INTO ONE CSV FILE HAS BEEN COMMENTED OUT, NOT \n",
    "# TO RISK OVERWRITING THE all_samples.csv FILE\n",
    "\n",
    "# Saving the result to csv, where all samples are gonna be saved\n",
    "# df_final = df_eim[['Sample', 'EIMMagnitude', 'EIMPhase', 'JointAngle', 'Mass', 'Time', 'RollingAverageMag', 'RollingAveragePhase']]\n",
    "# df_eim_kin = 'all_samples.csv'\n",
    "# \n",
    "# if(os.path.isfile(df_eim_kin)):\n",
    "#     sample_number = input(\"Please provide the sample number and press enter:\")\n",
    "#     mass = input(\"Please provide the mass used in the current sample:\")\n",
    "#     df_final = df_final.assign(Sample=sample_number)\n",
    "#     df_final = df_final.assign(Mass=mass)\n",
    "#     # df_final.loc[:]['Sample'] = sample_number\n",
    "#     # df_final.loc[:]['Mass'] = mass\n",
    "#     # df_final['Sample'] = df_final['Sample'].replace(df_final['Sample'], sample_number)\n",
    "#     # df_final['Mass'] = df_final['Mass'].replace(df_final['Mass'], sample_number)\n",
    "\n",
    "#     df_final.to_csv(df_eim_kin, mode='a', index= False, header=False)\n",
    "# else:\n",
    "#     sample_number = input(\"Please provide the sample number and press enter:\")\n",
    "#     mass = input(\"Please provide the mass used in the current sample:\")\n",
    "#     df_final = df_final.assign(Sample=sample_number)\n",
    "#     df_final = df_final.assign(Mass=mass)\n",
    "#     # df_final['Sample'] = df_final['Sample'].replace(df_final['Sample'], sample_number)\n",
    "#     # df_final['Mass'] = df_final['Mass'].replace(df_final['Mass'], sample_number)\n",
    "\n",
    "#     df_final.to_csv(df_eim_kin, mode='w', index= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming df_eim is your DataFrame\n",
    "plt.plot(df_eim['Timestamp'], df_eim['JointAngle'], marker='o')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('ElbowAngles')\n",
    "plt.title('Line Plot of ElbowAngles')\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(df_eim['Timestamp'], df_eim['EIMMagnitude'], marker='o')\n",
    "# plt.xlabel('Timestamp')\n",
    "# plt.ylabel('EIMMagnitude')\n",
    "# plt.title('Line Plot of EIMMagnitude')\n",
    "# plt.show()\n",
    "\n",
    "plt.plot(df_eim['Timestamp'], df_eim['RollingAverageMag'], marker='o')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('RollingAverageEIM')\n",
    "plt.title('Line Plot of RollingAverageEIM')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(df_eim['Timestamp'], df_eim['RollingStdEIM'], marker='o')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('RollingStdEIM')\n",
    "plt.title('Line Plot of RollingStdEIM')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(df_eim['Timestamp'], df_eim['RollingAveragePhase'], marker='o')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('EIMPhase')\n",
    "plt.title('Line Plot of EIMPhase')\n",
    "plt.show()\n",
    "\n",
    "# print(df_final.head(100))\n",
    "print(df_eim.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>From one csv file</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load csv of all samples. Group data by 'Sample'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_all = 'all_samples.csv'\n",
    "df_all=pd.read_csv(dir_all, encoding='utf-8')\n",
    "grouped = df_all.groupby('Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature engineering</h1> \n",
    "Calculating median, mean, standard deviation, variance and kurtosis for each group.\n",
    "\n",
    "1. **Kurtosis:**\n",
    "   - **Definition:** Kurtosis is a statistical measure that describes the shape of the distribution of a dataset. It measures the \"tailedness\" of the data distribution, indicating whether the data is heavy-tailed (more extreme values) or light-tailed (few extreme values).\n",
    "   - **Interpretation:**\n",
    "     - **Positive Kurtosis:** Indicates that the distribution has heavy tails and a sharp peak. It suggests that the data has more outliers or extreme values.\n",
    "     - **Negative Kurtosis:** Indicates that the distribution has light tails and a flat peak. It suggests that the data has fewer outliers and is more concentrated around the mean.\n",
    "     - **Zero Kurtosis:** The distribution is mesokurtic, similar to a normal distribution.\n",
    "   - **Formula:** Kurtosis is often defined as \\( \\frac{M_4}{\\sigma^4} - 3 \\), where \\( M_4 \\) is the fourth moment about the mean and \\( \\sigma \\) is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas options to display more columns\n",
    "pd.options.display.max_columns=150\n",
    "\n",
    "# Calculate the median for each group\n",
    "median_values = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].apply(lambda group: group[['EIMMagnitude', 'EIMPhase', 'JointAngle']].agg(['min', 'max']).median())\n",
    "\n",
    "# Calculate the mean for each group\n",
    "mean_values = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].mean()\n",
    "\n",
    "# Calculate the standard deviation for each group\n",
    "standard_deviations = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].std()\n",
    "\n",
    "# Calculate the variance for each group\n",
    "variance_values = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].var()\n",
    "\n",
    "# Calculate the kurtosis for each group\n",
    "kurtosis_values = grouped[['EIMMagnitude', 'EIMPhase', 'JointAngle']].apply(pd.DataFrame.kurtosis)\n",
    "\n",
    "# Reset the index to get the 'Sample' column back\n",
    "median_values.reset_index(inplace=True)\n",
    "mean_values.reset_index(inplace=True)\n",
    "standard_deviations.reset_index(inplace=True)\n",
    "variance_values.reset_index(inplace=True)\n",
    "kurtosis_values.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns to indicate they represent the median\n",
    "median_values.columns = ['Sample', 'MedianEIMMagnitude', 'MedianEIMPhase', 'MedianJointAngle']\n",
    "mean_values.columns = ['Sample', 'MeanEIMMagnitude', 'MeanEIMPhase', 'MeanJointAngle']\n",
    "standard_deviations.columns = ['Sample', 'StdEIMMagnitude', 'StdEIMPhase', 'StdJointAngle']\n",
    "variance_values.columns = ['Sample', 'VarEIMMagnitude', 'VarEIMPhase', 'VarJointAngle']\n",
    "kurtosis_values.columns = ['Sample', 'KurtEIMMagnitude', 'KurtEIMPhase', 'KurtJointAngle']\n",
    "\n",
    "# Merge the median and mean values back into the original DataFrame based on the 'Sample' column\n",
    "df_all = df_all.merge(median_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(mean_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(standard_deviations, on='Sample', how='left')\n",
    "df_all = df_all.merge(variance_values, on='Sample', how='left')\n",
    "df_all = df_all.merge(kurtosis_values, on='Sample', how='left')\n",
    "\n",
    "# Calculate rate of change for each group\n",
    "df_all['ROCEIMMagnitude'] = df_all['RollingAverageMag'].pct_change()\n",
    "df_all['ROCEIMPhase'] = df_all['RollingAveragePhase'].pct_change()\n",
    "df_all['ROCJointAngle'] = df_all['JointAngle'].pct_change()\n",
    "\n",
    "#Filling NaN values out with the first mean value in the series\n",
    "ROC_value=df_all['ROCEIMMagnitude'][1]\n",
    "df_all['ROCEIMMagnitude'].fillna(value=ROC_value, inplace=True)\n",
    "ROC_value=df_all['ROCEIMPhase'][1]\n",
    "df_all['ROCEIMPhase'].fillna(value=ROC_value, inplace=True)\n",
    "ROC_value=df_all['ROCJointAngle'][1]\n",
    "df_all['ROCJointAngle'].fillna(value=ROC_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Visualization</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming your data is stored in a pandas DataFrame named 'df'\n",
    "# Filter the data for the first 10 samples\n",
    "df_last_10_samples = df_all[df_all['Sample'] >= 89]\n",
    "\n",
    "# Create a scatter plot for EIMMagnitude and JointAngle\n",
    "fig = go.Figure()\n",
    "\n",
    "for sample in df_last_10_samples['Sample'].unique():\n",
    "    sample_data = df_last_10_samples[df_last_10_samples['Sample'] == sample]\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['EIMMagnitude'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - EIMMagnitude'))\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['JointAngle'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - JointAngle'))\n",
    "\n",
    "fig.update_layout(title='EMG and Kinematic Data for the last 10 Samples',\n",
    "                  xaxis_title='Time Steps', yaxis_title='Value')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data preprocessing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing. Range of the scaler should be the same for x and y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of an original dataframe\n",
    "df2=df_all.copy()\n",
    "\n",
    "# Define the columns you want to normalize\n",
    "columns_to_normalize = ['EIMMagnitude', 'EIMPhase',\t'JointAngle', 'RollingAverageMag', 'RollingAveragePhase', \n",
    "                        'MedianEIMMagnitude', 'MedianEIMPhase', 'MedianJointAngle', 'MeanEIMMagnitude',\n",
    "                        'MeanEIMPhase', 'MeanJointAngle', 'StdEIMMagnitude', 'StdEIMPhase',\t'StdJointAngle',\n",
    "                        'VarEIMMagnitude', 'VarEIMPhase', 'VarJointAngle', 'KurtEIMMagnitude', 'KurtEIMPhase',\n",
    "                        'KurtJointAngle', 'ROCEIMMagnitude', 'ROCEIMPhase', 'ROCJointAngle']\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "df2[columns_to_normalize] = scaler.fit_transform(df_all[columns_to_normalize])\n",
    "\n",
    "# Show a snaphsot of data\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your data is stored in a pandas DataFrame named 'df'\n",
    "# Filter the data for the first 10 samples\n",
    "df_last_10_samples = df2[df2['Sample'] >= 89]\n",
    "\n",
    "# Create a scatter plot for EIMMagnitude and JointAngle\n",
    "fig = go.Figure()\n",
    "\n",
    "for sample in df_last_10_samples['Sample'].unique():\n",
    "    sample_data = df_last_10_samples[df_last_10_samples['Sample'] == sample]\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['EIMMagnitude'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - EIMMagnitude'))\n",
    "    fig.add_trace(go.Scatter(x=sample_data.index, y=sample_data['JointAngle'],\n",
    "                             mode='lines+markers', name=f'Sample {sample} - JointAngle'))\n",
    "\n",
    "fig.update_layout(title='EMG and Kinematic Data for the First 10 Samples',\n",
    "                  xaxis_title='Time Steps', yaxis_title='Value')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prepare sequences for GRU</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important for synchronizing the data with eachother with a common time variable. The \"time-variable\" from the dataset is therefor strictly speaking not needed for training the network.\n",
    "\n",
    "The time_steps variable is a hyper parameter, that determines how many prior time steps the network should take into account. If time_steps=10 for example, each input sequence will contain data from the previous 10 steps, and the model will try to learn patterns based on this history.\n",
    "\n",
    "Using a smaller number of time steps might help the model capture short-term dependencies, while a larger number could capture longer-term patterns. It's essential to experiment and choose a value that best fits the nature of the data and the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training and evaluating GRU model Here are a few things to highlight before we start.</h3>\n",
    "\n",
    "**Example:** We will use sequences of 18 months to predict the average temperatures for the next 18 months. \n",
    "\n",
    "We will split the data into two separate dataframes — one for training and the other for validation (out of time validation).\n",
    "\n",
    "Since we are creating a many-to-many prediction model, we need to use a slightly more complex encoder-decoder configuration. Both encoder and decoder are hidden GRU layers, with information passed from one to another via a repeat vector layer.\n",
    "\n",
    "A repeat vector is necessary when we want to have sequences of different lengths, e.g., a sequence of 18 months to predict the next 12 months. It ensures that we provide the right shape for a decoder layer. However, if your input and output sequences are of the same length as in my example, then you can also choose to set return_sequences=True in the encoder layer and remove the repeat vector. Note that we added a Bidirectional wrapper to GRU layers. It allows us to train the model in both directions, which sometimes produces better results. However, its use is optional.\n",
    "\n",
    "Also, we need to use a Time Distributed wrapper in the output layer to predict outputs for each timestep individually. Finally, I have used MinMaxScaling in this example because it has produced better results than the unscaled version. You can find both scaled and unscaled setups within Jupyter Notebooks in my GitHub repository (link available at the end of the article).\n",
    "\n",
    "First, let’s define a helper function to reshape the data to a 3D array required by GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_gru(datain, timestep, scaler, columns_to_use):\n",
    "    X_out, Y_out = None, None\n",
    "\n",
    "    for sample in datain['Sample'].unique():\n",
    "        datatmp = datain[datain['Sample'] == sample].copy()\n",
    "\n",
    "        for col in columns_to_use:\n",
    "            arr = datatmp[col].to_numpy()\n",
    "\n",
    "            # Scale using transform (using previously fitted scaler)\n",
    "            arr_scaled = scaler.transform(arr.reshape(-1, 1)).flatten()\n",
    "\n",
    "            cnt = 0\n",
    "            for t in range(0, len(arr_scaled) - 2 * timestep + 1):\n",
    "                cnt += 1\n",
    "                X_start = t\n",
    "                X_end = t + timestep\n",
    "                Y_start = t + timestep\n",
    "                Y_end = t + 2 * timestep\n",
    "\n",
    "                if X_out is None:\n",
    "                    X_out = arr_scaled[X_start:X_end].reshape(1, timestep, 1)\n",
    "                    Y_out = arr_scaled[Y_start:Y_end].reshape(1, timestep, 1)\n",
    "                else:\n",
    "                    X_out = np.append(X_out, arr_scaled[X_start:X_end].reshape(1, timestep, 1), axis=0)\n",
    "                    Y_out = np.append(Y_out, arr_scaled[Y_start:Y_end].reshape(1, timestep, 1), axis=0)\n",
    "\n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data and reshaping it for GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "timestep = 2\n",
    "\n",
    "# Split data into train, eval, and test dataframes\n",
    "train_ratio = 0.7\n",
    "eval_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Split the data\n",
    "df_train, df_temp = train_test_split(df_all, test_size=1 - train_ratio, random_state=42)\n",
    "df_eval, df_test = train_test_split(df_temp, test_size=test_ratio / (test_ratio + eval_ratio), random_state=42)\n",
    "\n",
    "# 'EIMMagnitude', 'EIMPhase',\t'JointAngle', 'Mass', 'RollingAverageMag', 'RollingAveragePhase', \n",
    "#                         'MedianEIMMagnitude', 'MedianEIMPhase', 'MedianJointAngle', 'MeanEIMMagnitude',\n",
    "#                         'MeanEIMPhase', 'MeanJointAngle', 'StdEIMMagnitude', 'StdEIMPhase',\t'StdJointAngle',\n",
    "#                         'VarEIMMagnitude', 'VarEIMPhase', 'VarJointAngle', 'KurtEIMMagnitude', 'KurtEIMPhase',\n",
    "#                         'KurtJointAngle', 'ROCEIMMagnitude', 'ROCEIMPhase', 'ROCJointAngle'\n",
    "\n",
    "columns_to_train_on = ['EIMMagnitude']\n",
    "\n",
    "# Use fit to train the scaler on the training data only, actual scaling will be done inside reshaping function\n",
    "scaler.fit(df_train[columns_to_train_on].values.reshape(-1, 1))\n",
    "\n",
    "# Use the reshaping function to reshape the data for GRU\n",
    "columns_to_use = columns_to_train_on\n",
    "\n",
    "X_train, Y_train = reshape_for_gru(df_train, timestep, scaler, columns_to_use)\n",
    "X_eval, Y_eval = reshape_for_gru(df_eval, timestep, scaler, columns_to_use)\n",
    "X_test, Y_test = reshape_for_gru(df_test, timestep, scaler, columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train_shape: ',X_train.shape)\n",
    "print('Y_train_shape: ',Y_train.shape)\n",
    "print('X_eval_shape: ',X_eval.shape)\n",
    "print('Y_eval_shape: ',Y_eval.shape)\n",
    "print('X_test_shape: ',X_test.shape)\n",
    "print('Y_test_shape: ',Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture you've provided is a sequence-to-sequence model based on a Bidirectional GRU (gated recurrent unit) network. It's a reasonable choice for sequence-to-sequence problems, including mapping kinematic data to EIM data. The structure of your model includes the following components:\n",
    "\n",
    "1. **Input Layer**: This layer takes the input sequences. The shape is correctly specified as `(X_train.shape[1], X_train.shape[2])`, which matches the shape of your training data. This is suitable for the input data you have.\n",
    "\n",
    "2. **Bidirectional GRU Encoder Layer**: This layer effectively encodes the input sequence. Using a Bidirectional GRU allows the model to capture information from both past and future time steps. The `units=32` parameter defines the number of hidden units in this layer. The choice of the number of units depends on the complexity of your problem and the amount of data. You can experiment with different values to find the best one.\n",
    "\n",
    "3. **Repeat Vector Layer**: This layer repeats the encoded vector to match the sequence length of the output. In your case, it repeats the encoder's output sequence to match the expected sequence length of the decoder.\n",
    "\n",
    "4. **Bidirectional GRU Decoder Layer**: This layer decodes the repeated vector into an output sequence. It also uses a Bidirectional GRU for enhanced modeling capabilities. Like the encoder, the number of `units` can be adjusted to find the best fit for your problem.\n",
    "\n",
    "5. **TimeDistributed Dense Layer**: This layer applies a Dense layer to each time step of the sequence. The activation function is set to 'linear', which makes it suitable for regression tasks, such as predicting numerical values.\n",
    "\n",
    "Your model architecture seems well-suited for your task. However, the effectiveness of the model depends on the specifics of your data, the problem you're solving, and the quality of the data. You may need to experiment with different hyperparameters, such as the number of units in the GRU layers, the number of layers, and the architecture of the model.\n",
    "\n",
    "If you have additional features like mass, you can include them as additional input features in the input data. Your model can handle multiple input features. Be sure to preprocess and normalize all the input data consistently. Also, consider monitoring the training process and validating the model's performance to ensure it's learning the mapping effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 3 - Specify the structure of a Neural Network\n",
    "model = Sequential(name=\"GRU-Model\") # Model\n",
    "model.add(Input(shape=(X_train.shape[1],X_train.shape[2]), name='Input-Layer')) # Input Layer - need to speicfy the shape of inputs\n",
    "model.add(Bidirectional(GRU(units=32, activation='tanh', recurrent_activation='sigmoid', stateful=False), name='Hidden-GRU-Encoder-Layer')) # Encoder Layer\n",
    "model.add(RepeatVector(X_train.shape[1], name='Repeat-Vector-Layer')) # Repeat Vector\n",
    "model.add(Bidirectional(GRU(units=32, activation='tanh', recurrent_activation='sigmoid', stateful=False, return_sequences=True), name='Hidden-GRU-Decoder-Layer')) # Decoder Layer\n",
    "model.add(TimeDistributed(Dense(units=1, activation='linear'), name='Output-Layer')) # Output Layer, Linear(x) = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model compilation seems reasonable for your problem. Here's a breakdown of the compilation settings:\n",
    "\n",
    "1. **Optimizer**: You've chosen 'adam' as the optimizer. Adam is a popular and effective optimizer for training neural networks. It's known for its adaptive learning rate and tends to work well in a wide range of tasks. However, the choice of the optimizer can also depend on your specific problem and dataset. It's often a good starting point.\n",
    "\n",
    "2. **Loss Function**: You've chosen 'mean_squared_error' as the loss function. This is a suitable choice for regression tasks, where the goal is to predict numerical values. It measures the mean of the squared differences between the predicted and actual values. The choice of loss function aligns with your problem of mapping kinematic data to EIM data.\n",
    "\n",
    "3. **Metrics**: You've selected 'MeanSquaredError' and 'MeanAbsoluteError' as metrics. These are relevant metrics for regression tasks, helping you evaluate how well your model's predictions match the actual data. 'MeanSquaredError' measures the average of the squared differences, while 'MeanAbsoluteError' measures the average of the absolute differences.\n",
    "\n",
    "4. **Loss Weights**: You've left this as `None`. Loss weights can be useful when you have multiple outputs with different importance levels. If your model has only one output (e.g., predicting EIM data), this isn't needed. If you have multiple outputs with varying importance, you might consider using loss weights.\n",
    "\n",
    "5. **Weighted Metrics**: This is set to `None`. Weighted metrics are useful when different samples or classes in your data have varying importance. If you don't have specific weighting requirements, you can leave this as is.\n",
    "\n",
    "6. **Run Eagerly**: It's set to `None`, which means it uses the default behavior. TensorFlow can compile models to a more efficient format using tf.function, but it might be necessary to disable this for some custom model logic. Leaving it as `None` is usually fine unless you run into specific issues that require disabling tf.function.\n",
    "\n",
    "7. **Steps per Execution**: This parameter can affect performance on certain hardware, but its default value is `1`, which is suitable for most cases. It determines how many batches to run in each tf.function call. You can experiment with different values if you're optimizing for performance on specific hardware, but for general use, the default value should work.\n",
    "\n",
    "Overall, the model compilation settings appear appropriate for your problem. However, model training and evaluation will provide better insights into its performance. You can experiment with different optimizers, loss functions, and metrics during the model's training to fine-tune its performance based on your dataset and specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 4 - Compile the model\n",
    "model.compile(optimizer='adam', # default='rmsprop', an algorithm to be used in backpropagation\n",
    "              loss='mean_squared_error', # Loss function to be optimized. A string (name of loss function), or a tf.keras.losses.Loss instance.\n",
    "              metrics=['MeanSquaredError', 'MeanAbsoluteError'], # List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance.\n",
    "              loss_weights=None, # default=None, Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs.\n",
    "              weighted_metrics=None, # default=None, List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing.\n",
    "              run_eagerly=None, # Defaults to False. If True, this Model's logic will not be wrapped in a tf.function. Recommended to leave this as None unless your Model cannot be run inside a tf.function.\n",
    "              steps_per_execution=None # Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead.\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters used in your model fit seem reasonable, but a few considerations and potential adjustments are worth noting:\n",
    "\n",
    "1. **Batch Size**: You've set `batch_size=1`, which means you're using a batch size of 1 sample per gradient update. This can be very slow, especially if you have a large dataset. Larger batch sizes (e.g., 16, 32, or even larger) are often used to take advantage of parallel processing and speed up training. Smaller batch sizes may be suitable if you have memory constraints.\n",
    "\n",
    "2. **Validation Split**: You've set `validation_split=0.2`, which allocates 20% of your training data for validation. This is a common practice, but you also have the option to provide explicit validation data using `validation_data=(X_test, Y_test)`. Providing explicit validation data can be helpful if you want to monitor the model's performance on a specific dataset.\n",
    "\n",
    "3. **Epochs**: You've set `epochs=50`, which determines how many times the model will iterate over the entire training dataset. The number of epochs depends on your problem and dataset. It's a good practice to monitor the model's performance during training and stop when it no longer improves significantly.\n",
    "\n",
    "4. **Callbacks**: You've left `callbacks` as `None`. Callbacks are powerful tools for customizing the training process. Common callbacks include early stopping, model checkpointing, and learning rate scheduling. Depending on your problem, you might want to consider using callbacks to improve training efficiency.\n",
    "\n",
    "5. **Shuffle**: You've set `shuffle=True`, which shuffles the training data before each epoch. This is typically recommended to ensure that the model generalizes well. However, if your data has a temporal sequence (like time series data), shuffling might not be appropriate, and you may want to set `shuffle=False`.\n",
    "\n",
    "6. **Workers and Multiprocessing**: You've set `workers=1` and `use_multiprocessing=True`. These parameters are used for generator-based data input. The number of workers can be increased to parallelize data loading if you're working with large datasets. The decision to use multiprocessing can depend on your system's capabilities.\n",
    "\n",
    "7. **Initial Epoch**: You've set `initial_epoch=0`, which means training starts from the first epoch. If you want to continue training from a saved model or resume training from a specific point, you can set this to a different value.\n",
    "\n",
    "8. **Validation Frequency**: You've set `validation_freq=10`, which means validation is performed every 10 epochs. This is a good practice to reduce computational overhead. Adjust this based on how frequently you want to validate your model.\n",
    "\n",
    "The fit parameters you've provided are a good starting point. However, you may need to fine-tune some of these settings based on your specific problem and the behavior of your model during training. It's also a good practice to monitor training and validation metrics to ensure that the model is learning effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 5 - Fit the model on the dataset\n",
    "history = model.fit(X_train, # input data\n",
    "                    Y_train, # target data\n",
    "                    batch_size=32, # Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
    "                    epochs=50, # default=1, Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided\n",
    "                    verbose=1, # default='auto', ('auto', 0, 1, or 2). Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy.\n",
    "                    callbacks=None, # default=None, list of callbacks to apply during training. See tf.keras.callbacks\n",
    "                    # validation_split=0.2, # default=0.0, Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch.\n",
    "                    validation_data=(X_eval, Y_eval), # default=None, Data on which to evaluate the loss and any model metrics at the end of each epoch.\n",
    "                    shuffle=False, # default=True, Boolean (whether to shuffle the training data before each epoch) or str (for 'batch').\n",
    "                    class_weight=None, # default=None, Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n",
    "                    sample_weight=None, # default=None, Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only).\n",
    "                    initial_epoch=0, # Integer, default=0, Epoch at which to start training (useful for resuming a previous training run).\n",
    "                    steps_per_epoch=None, # Integer or None, default=None, Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined.\n",
    "                    validation_steps=None, # Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch.\n",
    "                    validation_batch_size=None, # Integer or None, default=None, Number of samples per validation batch. If unspecified, will default to batch_size.\n",
    "                    validation_freq=10, # default=1, Only relevant if validation data is provided. If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs.\n",
    "                    max_queue_size=10, # default=10, Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10.\n",
    "                    workers=1, # default=1, Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1.\n",
    "                    use_multiprocessing=True, # default=False, Used for generator or keras.utils.Sequence input only. If True, use process-based threading. If unspecified, use_multiprocessing will default to False.\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 6 - Use model to make predictions\n",
    "# Predict results on training data\n",
    "#pred_train = model.predict(X_train)\n",
    "# Predict results on test data\n",
    "pred_test = model.predict(X_test)\n",
    "\n",
    "\n",
    "##### Step 7 - Print Performance Summary\n",
    "print(\"\")\n",
    "print('-------------------- Model Summary --------------------')\n",
    "model.summary() # print model summary\n",
    "print(\"\")\n",
    "print('-------------------- Weights and Biases --------------------')\n",
    "print(\"Too many parameters to print but you can use the code provided if needed\")\n",
    "print(\"\")\n",
    "#for layer in model.layers:\n",
    "#    print(layer.name)\n",
    "#    for item in layer.get_weights():\n",
    "#        print(\"  \", item)\n",
    "#print(\"\")\n",
    "\n",
    "# Print the last value in the evaluation metrics contained within history file\n",
    "print('-------------------- Evaluation on Training Data --------------------')\n",
    "for item in history.history:\n",
    "    print(\"Final\", item, \":\", history.history[item][-1])\n",
    "print(\"\")\n",
    "\n",
    "# Evaluate the model on the test data using \"evaluate\"\n",
    "print('-------------------- Evaluation on Test Data --------------------')\n",
    "results = model.evaluate(X_test, Y_test)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "\n",
    "# Create a plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for actual values (ground truth)\n",
    "for i in range(12):\n",
    "    fig.add_trace(go.Scatter(x=np.arange(len(Y_test[i])), y=scaler.inverse_transform(Y_test[i].reshape(-1, 1)).flatten(),\n",
    "                             mode='lines',\n",
    "                             name=f'Sample {i + 1} - Actual',\n",
    "                             opacity=0.8,\n",
    "                             line=dict(width=1)\n",
    "                            ))\n",
    "\n",
    "# Add traces for predicted values\n",
    "for i in range(12):\n",
    "    fig.add_trace(go.Scatter(x=np.arange(len(pred_test[i])), y=scaler.inverse_transform(pred_test[i].reshape(-1, 1)).flatten(),\n",
    "                             mode='lines',\n",
    "                             name=f'Sample {i + 1} - Predicted',\n",
    "                             opacity=1,\n",
    "                             line=dict(width=2, dash='dot')\n",
    "                            ))\n",
    "\n",
    "# Customize the plot appearance\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white',\n",
    "    xaxis=dict(\n",
    "        showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "        zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "        showline=True, linewidth=1, linecolor='black',\n",
    "        title='Time Steps'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "        zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "        showline=True, linewidth=1, linecolor='black',\n",
    "        title='Values'\n",
    "    ),\n",
    "    title=dict(text=\"Actual vs. Predicted Values\", font=dict(color='black'))\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Splitting the data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the data into a training/evaluation/test distribution of 70/20/10. The random_state parameter is a seed for the random split, that allows reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kinematic_features = 18\n",
    "num_emg_features = 7\n",
    "\n",
    "# Extract X_kinematic and y_kinematic\n",
    "X_kinematic = kinematic_data_normalized[:, :, :num_kinematic_features]  # Adjust num_kinematic_features\n",
    "y_kinematic = kinematic_data_normalized[:, -1, :]  # Assuming the last time step represents the target\n",
    "\n",
    "# Extract X_emg and y_emg\n",
    "X_emg = emg_data_normalized[:, :, :num_emg_features]  # Adjust num_emg_features\n",
    "y_emg = emg_data_normalized[:, -1, :]  # Assuming the last time step represents the target\n",
    "\n",
    "# Now you have X_kinematic, y_kinematic, X_emg, and y_emg for further processing\n",
    "\n",
    "# Split the data\n",
    "X_kinematic_train, X_kinematic_val, y_kinematic_train, y_kinematic_val = train_test_split(\n",
    "    X_kinematic, y_kinematic, test_size=0.3, random_state=42\n",
    ")\n",
    "X_emg_train, X_emg_val, y_emg_train, y_emg_val = train_test_split(\n",
    "    X_emg, y_emg, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Ensure shapes match the model input requirements\n",
    "# Add additional processing steps if necessary\n",
    "\n",
    "# Print the shapes for verification\n",
    "# print(\"Shapes of Kinematic Data Sets:\")\n",
    "# print(\"Train:\", X_train_kinematic.shape, y_train_kinematic.shape)\n",
    "# print(\"Validation:\", X_val_kinematic.shape, y_val_kinematic.shape)\n",
    "# print(\"Test:\", X_test_kinematic.shape, y_test_kinematic.shape)\n",
    "\n",
    "# print(\"\\nShapes of EMG Data Sets:\")\n",
    "# print(\"Train:\", X_train_emg.shape, y_train_emg.shape)\n",
    "# print(\"Validation:\", X_val_emg.shape, y_val_emg.shape)\n",
    "# print(\"Test:\", X_test_emg.shape, y_test_emg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model architecture</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flexibility:** Using a Seq2Seq (sequence to sequence) architecture with GRU as RNN's, since the length of the EIM signals might vary, and Seq2Seq can handle this. Normal GRU would require to truncate the samples to the same lengths. The Seq2Seq model provides more flexibility in capturing complex relationships between variable-length sequences, making it potentially more suitable for tasks where the alignment between input and output elements is not fixed.\n",
    "\n",
    "**Attention mechanism:** The attention mechanism in the Seq2Seq model allows the network to focus on different parts of the input sequence when making predictions for each element in the output sequence. This is beneficial when the relationship between EMG signals and joint angles might not be straightforward and might require the model to attend to different segments of the input sequence.\n",
    "\n",
    "Splits the EMG and kinematic data into two sepparate branches to train on. The idea is to create a model that can learn the relationship between EMG data and joint angles by processing both types of data during training.\n",
    "\n",
    "During the training phase, the model learns to capture patterns and relationships between the provided EMG signals and the corresponding joint angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Masking, RepeatVector, Layer, Reshape\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Assuming num_emg_features, num_kinematic_features, and num_joint_angle_features are defined\n",
    "num_emg_features = 7\n",
    "num_kinematic_features = 18\n",
    "\n",
    "# Total number of features\n",
    "num_input_features = num_emg_features + num_kinematic_features\n",
    "\n",
    "# Define the input sequence shape\n",
    "input_seq_shape = (None, num_input_features)  # Variable-length input sequence\n",
    "\n",
    "# Define the GRU units\n",
    "gru_units = 32\n",
    "\n",
    "# EMG data branch\n",
    "encoder_inputs_emg = Input(shape=(None, num_emg_features), name='Input-Layer-EMG')\n",
    "encoder_emg = GRU(gru_units, return_sequences=True, name='Hidden-GRU-Encoder-Layer-EMG')(encoder_inputs_emg)\n",
    "\n",
    "# Kinematic data branch\n",
    "encoder_inputs_kinematic = Input(shape=(None, num_kinematic_features), name='Input-Layer-Kinematic')\n",
    "encoder_kinematic = GRU(gru_units, return_sequences=True, name='Hidden-GRU-Encoder-Layer-Kinematic')(encoder_inputs_kinematic)\n",
    "\n",
    "# Concatenate EMG and kinematic encodings\n",
    "encoder_combined = concatenate([encoder_emg, encoder_kinematic], axis=-1)\n",
    "\n",
    "# Attention Mechanism\n",
    "attention = Dot(axes=[1, 1], name='Attention-Layer')([encoder_combined, encoder_combined])\n",
    "attention = Activation('softmax', name='Attention-Activation')(attention)\n",
    "\n",
    "# Apply attention weights to encoder outputs\n",
    "context = Dot(axes=[1, 2], name='Context-Layer')([attention, encoder_combined])\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None, num_kinematic_features), name='Decoder-Input-Layer')\n",
    "decoder_gru = GRU(gru_units, return_sequences=True, name='Hidden-GRU-Decoder-Layer')(decoder_inputs)\n",
    "\n",
    "# Flatten context along with masking\n",
    "context_flattened = Flatten()(Masking()(context))\n",
    "\n",
    "# Custom layer to repeat the context along the time axis\n",
    "class RepeatContextLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RepeatContextLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.expand_dims(inputs, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 1, input_shape[1])\n",
    "\n",
    "context_expanded = RepeatContextLayer(name='Repeat-Context')(context_flattened)\n",
    "\n",
    "# Concatenate expanded context and decoder GRU output\n",
    "decoder_combined = Concatenate(axis=-1, name='Concatenate-Layer')([context_expanded, decoder_gru])\n",
    "\n",
    "# Flatten the decoder_combined while maintaining the sequence length\n",
    "decoder_flattened = Flatten(name='Flatten-Layer')(decoder_combined)\n",
    "\n",
    "print('Decoder shape before reshaping: ', decoder_flattened.shape)\n",
    "# Reshape to have a single dimension in the output\n",
    "decoder_output = Reshape((-1, 1), name='Reshape-Layer')(decoder_flattened)\n",
    "print('Flatten-Layer shape: ', decoder_output.shape)\n",
    "\n",
    "# Output layer\n",
    "outputs = TimeDistributed(Dense(1, activation='linear'), name='Output-Layer')(decoder_output)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[encoder_inputs_emg, encoder_inputs_kinematic, decoder_inputs], outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Print model summary for review\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# model.fit(X_train_kinematic, y_train_kinematic, epochs=10, batch_size=32, validation_data=(X_test_kinematic, y_test_kinematic))\n",
    "\n",
    "# Use EarlyStopping to stop training when the validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model on both EMG and kinematic data\n",
    "# Set the number of epochs and batch size\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Training the model\n",
    "model.fit(\n",
    "    [X_train_emg, X_train_kinematic],  # Only EMG and kinematic inputs during training\n",
    "    [y_train_kinematic, y_train_kinematic[:, 1:]],  # y_train_kinematic[:, 1:] is the shifted version for training the decoder,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=([X_val_emg, X_val_kinematic], y_val_kinematic),  # Adjust for validation data\n",
    "    callbacks=[early_stopping]  # If you are using EarlyStopping\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate([X_test_emg, X_test_kinematic], y_test_kinematic)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three months are missing from the dataset, so average from preceding and subsequent months are calculated as placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add missing months 2011-04, 2011-04, 2011-04 and impute data\n",
    "df2_pivot['2011-04']=(df2_pivot['2011-03']+df2_pivot['2011-05'])/2\n",
    "df2_pivot['2012-12']=(df2_pivot['2012-11']+df2_pivot['2013-01'])/2\n",
    "df2_pivot['2013-02']=(df2_pivot['2013-01']+df2_pivot['2013-03'])/2\n",
    "\n",
    "# Sort columns so Year-Months are in the correct order\n",
    "df2_pivot=df2_pivot.reindex(sorted(df2_pivot.columns), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nbformat\n",
    "\n",
    "# Plot average monthly temperature derived from daily medians for each location\n",
    "fig = go.Figure()\n",
    "for location in df2_pivot.index:\n",
    "    fig.add_trace(go.Scatter(x=df2_pivot.loc[location, :].index,\n",
    "                             y=df2_pivot.loc[location, :].values,\n",
    "                             mode='lines',\n",
    "                             name=location,\n",
    "                             opacity=0.8,\n",
    "                             line=dict(width=1)\n",
    "                            ))\n",
    "\n",
    "# Change chart background color\n",
    "fig.update_layout(dict(plot_bgcolor = 'white'), showlegend=True)\n",
    "\n",
    "# Update axes lines\n",
    "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "                 showline=True, linewidth=1, linecolor='black',\n",
    "                 title='Date'\n",
    "                )\n",
    "\n",
    "fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "                 showline=True, linewidth=1, linecolor='black',\n",
    "                 title='Degrees Celsius'\n",
    "                )\n",
    "\n",
    "# Set figure title\n",
    "fig.update_layout(title=dict(text=\"Average Monthly Temperatures\", font=dict(color='black')))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the mean temperature, as well as variation, differs between locations. We can either train a location-specific model for better precision or a generic model to predict temperatures for every area.\n",
    "\n",
    "In this example, I will create a generic model trained on all locations.\n",
    "\n",
    "Training and evaluating GRU model Here are a few things to highlight before we start.\n",
    "\n",
    "We will use sequences of 18 months to predict the average temperatures for the next 18 months. You can adjust that to your liking but beware that there will not be enough data for sequences beyond 23 months in length.\n",
    "\n",
    "We will split the data into two separate dataframes — one for training and the other for validation (out of time validation).\n",
    "\n",
    "Since we are creating a many-to-many prediction model, we need to use a slightly more complex encoder-decoder configuration. Both encoder and decoder are hidden GRU layers, with information passed from one to another via a repeat vector layer.\n",
    "\n",
    "A repeat vector is necessary when we want to have sequences of different lengths, e.g., a sequence of 18 months to predict the next 12 months. It ensures that we provide the right shape for a decoder layer. However, if your input and output sequences are of the same length as in my example, then you can also choose to set return_sequences=True in the encoder layer and remove the repeat vector. Note that we added a Bidirectional wrapper to GRU layers. It allows us to train the model in both directions, which sometimes produces better results. However, its use is optional.\n",
    "\n",
    "Also, we need to use a Time Distributed wrapper in the output layer to predict outputs for each timestep individually. Finally, I have used MinMaxScaling in this example because it has produced better results than the unscaled version. You can find both scaled and unscaled setups within Jupyter Notebooks in my GitHub repository (link available at the end of the article).\n",
    "\n",
    "First, let’s define a helper function to reshape the data to a 3D array required by GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shaping(datain, timestep, scaler):\n",
    "\n",
    "    # Loop through each location\n",
    "    for location in datain.index:\n",
    "        datatmp = datain[datain.index==location].copy()\n",
    "\n",
    "        # Convert input dataframe to array and flatten\n",
    "        arr=datatmp.to_numpy().flatten()\n",
    "\n",
    "        # Scale using transform (using previously fitted scaler)\n",
    "        arr_scaled=scaler.transform(arr.reshape(-1, 1)).flatten()\n",
    "\n",
    "        cnt=0\n",
    "        for mth in range(0, len(datatmp.columns)-(2*timestep)+1): # Define range\n",
    "            cnt=cnt+1 # Gives us the number of samples. Later used to reshape the data\n",
    "            X_start=mth # Start month for inputs of each sample\n",
    "            X_end=mth+timestep # End month for inputs of each sample\n",
    "            Y_start=mth+timestep # Start month for targets of each sample. Note, start is inclusive and end is exclusive, that's why X_end and Y_start is the same number\n",
    "            Y_end=mth+2*timestep # End month for targets of each sample.\n",
    "\n",
    "            # Assemble input and target arrays containing all samples\n",
    "            if mth==0:\n",
    "                X_comb=arr_scaled[X_start:X_end]\n",
    "                Y_comb=arr_scaled[Y_start:Y_end]\n",
    "            else:\n",
    "                X_comb=np.append(X_comb, arr_scaled[X_start:X_end])\n",
    "                Y_comb=np.append(Y_comb, arr_scaled[Y_start:Y_end])\n",
    "\n",
    "        # Reshape input and target arrays\n",
    "        X_loc=np.reshape(X_comb, (cnt, timestep, 1))\n",
    "        Y_loc=np.reshape(Y_comb, (cnt, timestep, 1))\n",
    "\n",
    "        # Append an array for each location to the master array\n",
    "        if location==datain.index[0]:\n",
    "            X_out=X_loc\n",
    "            Y_out=Y_loc\n",
    "        else:\n",
    "            X_out=np.concatenate((X_out, X_loc), axis=0)\n",
    "            Y_out=np.concatenate((Y_out, Y_loc), axis=0)\n",
    "\n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train GRU neural network over 50 epochs and display the model summary with evaluation metrics. You can follow my comments within the code to understand each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 1 - Specify parameters\n",
    "timestep=18\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 2 - Prepare data\n",
    "\n",
    "# Split data into train and test dataframes\n",
    "df_train=df2_pivot.iloc[:, 0:-2*timestep].copy()\n",
    "df_test=df2_pivot.iloc[:, -2*timestep:].copy()\n",
    "\n",
    "# Use fit to train the scaler on the training data only, actual scaling will be done inside reshaping function\n",
    "scaler.fit(df_train.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Use previously defined shaping function to reshape the data for GRU\n",
    "X_train, Y_train = shaping(datain=df_train, timestep=timestep, scaler=scaler)\n",
    "X_test, Y_test = shaping(datain=df_test, timestep=timestep, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 3 - Specify the structure of a Neural Network\n",
    "model = Sequential(name=\"GRU-Model\") # Model\n",
    "model.add(Input(shape=(X_train.shape[1],X_train.shape[2]), name='Input-Layer')) # Input Layer - need to speicfy the shape of inputs\n",
    "model.add(Bidirectional(GRU(units=32, activation='tanh', recurrent_activation='sigmoid', stateful=False), name='Hidden-GRU-Encoder-Layer')) # Encoder Layer\n",
    "model.add(RepeatVector(X_train.shape[1], name='Repeat-Vector-Layer')) # Repeat Vector\n",
    "model.add(Bidirectional(GRU(units=32, activation='tanh', recurrent_activation='sigmoid', stateful=False, return_sequences=True), name='Hidden-GRU-Decoder-Layer')) # Decoder Layer\n",
    "model.add(TimeDistributed(Dense(units=1, activation='linear'), name='Output-Layer')) # Output Layer, Linear(x) = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 4 - Compile the model\n",
    "model.compile(optimizer='adam', # default='rmsprop', an algorithm to be used in backpropagation\n",
    "              loss='mean_squared_error', # Loss function to be optimized. A string (name of loss function), or a tf.keras.losses.Loss instance.\n",
    "              metrics=['MeanSquaredError', 'MeanAbsoluteError'], # List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance.\n",
    "              loss_weights=None, # default=None, Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs.\n",
    "              weighted_metrics=None, # default=None, List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing.\n",
    "              run_eagerly=None, # Defaults to False. If True, this Model's logic will not be wrapped in a tf.function. Recommended to leave this as None unless your Model cannot be run inside a tf.function.\n",
    "              steps_per_execution=None # Defaults to 1. The number of batches to run during each tf.function call. Running multiple batches inside a single tf.function call can greatly improve performance on TPUs or small models with a large Python overhead.\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 5 - Fit the model on the dataset\n",
    "history = model.fit(X_train, # input data\n",
    "                    Y_train, # target data\n",
    "                    batch_size=1, # Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
    "                    epochs=50, # default=1, Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided\n",
    "                    verbose=1, # default='auto', ('auto', 0, 1, or 2). Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy.\n",
    "                    callbacks=None, # default=None, list of callbacks to apply during training. See tf.keras.callbacks\n",
    "                    validation_split=0.2, # default=0.0, Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch.\n",
    "                    #validation_data=(X_test, y_test), # default=None, Data on which to evaluate the loss and any model metrics at the end of each epoch.\n",
    "                    shuffle=True, # default=True, Boolean (whether to shuffle the training data before each epoch) or str (for 'batch').\n",
    "                    class_weight=None, # default=None, Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n",
    "                    sample_weight=None, # default=None, Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only).\n",
    "                    initial_epoch=0, # Integer, default=0, Epoch at which to start training (useful for resuming a previous training run).\n",
    "                    steps_per_epoch=None, # Integer or None, default=None, Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined.\n",
    "                    validation_steps=None, # Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch.\n",
    "                    validation_batch_size=None, # Integer or None, default=None, Number of samples per validation batch. If unspecified, will default to batch_size.\n",
    "                    validation_freq=10, # default=1, Only relevant if validation data is provided. If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs.\n",
    "                    max_queue_size=10, # default=10, Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10.\n",
    "                    workers=1, # default=1, Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1.\n",
    "                    use_multiprocessing=True, # default=False, Used for generator or keras.utils.Sequence input only. If True, use process-based threading. If unspecified, use_multiprocessing will default to False.\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Step 6 - Use model to make predictions\n",
    "# Predict results on training data\n",
    "#pred_train = model.predict(X_train)\n",
    "# Predict results on test data\n",
    "pred_test = model.predict(X_test)\n",
    "\n",
    "\n",
    "##### Step 7 - Print Performance Summary\n",
    "print(\"\")\n",
    "print('-------------------- Model Summary --------------------')\n",
    "model.summary() # print model summary\n",
    "print(\"\")\n",
    "print('-------------------- Weights and Biases --------------------')\n",
    "print(\"Too many parameters to print but you can use the code provided if needed\")\n",
    "print(\"\")\n",
    "#for layer in model.layers:\n",
    "#    print(layer.name)\n",
    "#    for item in layer.get_weights():\n",
    "#        print(\"  \", item)\n",
    "#print(\"\")\n",
    "\n",
    "# Print the last value in the evaluation metrics contained within history file\n",
    "print('-------------------- Evaluation on Training Data --------------------')\n",
    "for item in history.history:\n",
    "    print(\"Final\", item, \":\", history.history[item][-1])\n",
    "print(\"\")\n",
    "\n",
    "# Evaluate the model on the test data using \"evaluate\"\n",
    "print('-------------------- Evaluation on Test Data --------------------')\n",
    "results = model.evaluate(X_test, Y_test)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s regenerate predictions for the 5 locations we picked earlier and plot the results on a chart to compare actual and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select locations to predict temperatures for\n",
    "location=['Cairns', 'Canberra', 'Darwin', 'GoldCoast', 'MountGinini']\n",
    "dfloc_test = df_test[df_test.index.isin(location)].copy()\n",
    "\n",
    "# Reshape test data\n",
    "X_test, Y_test = shaping(datain=dfloc_test, timestep=timestep, scaler=scaler)\n",
    "\n",
    "# Predict results on test data\n",
    "pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average monthly temperatures (actual and predicted) for test (out of time) data\n",
    "fig = go.Figure()\n",
    "\n",
    "# Trace for actual temperatures\n",
    "for location in dfloc_test.index:\n",
    "    fig.add_trace(go.Scatter(x=dfloc_test.loc[location, :].index,\n",
    "                             y=dfloc_test.loc[location, :].values,\n",
    "                             mode='lines',\n",
    "                             name=location,\n",
    "                             opacity=0.8,\n",
    "                             line=dict(width=1)\n",
    "                            ))\n",
    "\n",
    "# Trace for predicted temperatures\n",
    "for i in range(0,pred_test.shape[0]):\n",
    "    fig.add_trace(go.Scatter(x=np.array(dfloc_test.columns[-timestep:]),\n",
    "                             # Need to inverse transform the predictions before plotting\n",
    "                             y=scaler.inverse_transform(pred_test[i].reshape(-1,1)).flatten(),\n",
    "                             mode='lines',\n",
    "                             name=dfloc_test.index[i]+' Prediction',\n",
    "                             opacity=1,\n",
    "                             line=dict(width=2, dash='dot')\n",
    "                            ))\n",
    "\n",
    "# Change chart background color\n",
    "fig.update_layout(dict(plot_bgcolor = 'white'))\n",
    "\n",
    "# Update axes lines\n",
    "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "                 showline=True, linewidth=1, linecolor='black',\n",
    "                 title='Year-Month'\n",
    "                )\n",
    "\n",
    "fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey',\n",
    "                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey',\n",
    "                 showline=True, linewidth=1, linecolor='black',\n",
    "                 title='Degrees Celsius'\n",
    "                )\n",
    "\n",
    "# Set figure title\n",
    "fig.update_layout(title=dict(text=\"Average Monthly Temperatures\", font=dict(color='black')))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
